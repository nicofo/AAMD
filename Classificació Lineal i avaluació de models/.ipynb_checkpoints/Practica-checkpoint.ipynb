{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importació de les dades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'ElemStatLearn' was built under R version 3.3.3\""
     ]
    }
   ],
   "source": [
    "library(ElemStatLearn)\n",
    "data(spam)\n",
    "wine<-read.csv('wine.csv',header=FALSE)\n",
    "colnames(wine)<-c(\"Type\",\"Alcohol\",\"Malic\",\"Ash\",\"Alcalinity\",\"Magnesium\",\"Phenols\",\"Flavonoids\",\"Nonflavonoids\",\"Proanthocyanins\",\"Color\",\"Hue\",\"Dilution\",\"Proline\")\n",
    "wine$Type <- as.factor(wine$Type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llibreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificació de Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripció de les dades **TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4601"
      ],
      "text/latex": [
       "4601"
      ],
      "text/markdown": [
       "4601"
      ],
      "text/plain": [
       "[1] 4601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem que el dataset és bastant gran aixi que utilitzarem com a model de predicció k-fold cross-validation. \n",
    "No utilitcem Leave-one-out per que seria poc eficient fer les 4601 fileres, tampoc utilitzarem boostrap ja que no tenim necessitat de ampliar la mostra.\n",
    "\n",
    "Utilitzarem un k de 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam<-spam[sample(nrow(spam)),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificació per  K-nn\n",
    "Utilitzarem la funció *knn* que trobem al paquet *class*.\n",
    "Amb la quantitat de dades que hi han testejarem amb una k= 10 i 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   224   36\n",
      "  spam     50  150\n",
      "preciso email  0.8615385\n",
      "precisio spam  0.75\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   255   38\n",
      "  spam     53  114\n",
      "preciso email  0.8703072\n",
      "precisio spam  0.6826347\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   228   62\n",
      "  spam     47  123\n",
      "preciso email  0.7862069\n",
      "precisio spam  0.7235294\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   245   36\n",
      "  spam     52  127\n",
      "preciso email  0.8718861\n",
      "precisio spam  0.7094972\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   224   46\n",
      "  spam     38  152\n",
      "preciso email  0.8296296\n",
      "precisio spam  0.8\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   239   55\n",
      "  spam     56  110\n",
      "preciso email  0.8129252\n",
      "precisio spam  0.6626506\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   230   44\n",
      "  spam     48  138\n",
      "preciso email  0.8394161\n",
      "precisio spam  0.7419355\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   240   43\n",
      "  spam     53  124\n",
      "preciso email  0.8480565\n",
      "precisio spam  0.700565\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   214   35\n",
      "  spam     62  149\n",
      "preciso email  0.8594378\n",
      "precisio spam  0.7061611\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   247   47\n",
      "  spam     46  120\n",
      "preciso email  0.8401361\n",
      "precisio spam  0.7228916\n",
      "\n",
      "preciso email mitjana  0.841954\n",
      "precisio spam mitjana  0.7199865"
     ]
    }
   ],
   "source": [
    "k<-10\n",
    "\n",
    "kfold<-10\n",
    "totalprecisio.email<-0\n",
    "totalprecisio.spam<-0\n",
    "for (i in 1:kfold){\n",
    "    ITest<-((nrow(spam)/kfold)*(i-1)+1):((nrow(spam)/kfold)*i)\n",
    "    spam.train<-spam[-ITest,-ncol(spam)]\n",
    "    spam.test<-spam[ITest,-ncol(spam)]\n",
    "    spam.Ytrain<-spam[-ITest,ncol(spam)]\n",
    "    spam.Ytest<-spam[ITest,ncol(spam)]\n",
    "    spam.Yhat<-knn(spam.train, spam.test, spam.Ytrain, k = k)\n",
    "    C<-table(\"True\"=spam.Ytest,\"Predicted\"=spam.Yhat)\n",
    "    cat('\\nMatriu confusio\\n')\n",
    "    print(C)\n",
    "    precisio.email<-C[1]/(C[1]+C[3])\n",
    "    precisio.spam<-C[4]/(C[2]+C[4])\n",
    "    cat('preciso email ',+precisio.email)\n",
    "    cat('\\nprecisio spam ',precisio.spam)\n",
    "    cat('\\n')\n",
    "    totalprecisio.email<-totalprecisio.email + precisio.email\n",
    "    totalprecisio.spam<-totalprecisio.spam + precisio.spam\n",
    "}\n",
    "cat('\\npreciso email mitjana ',totalprecisio.email/kfold)\n",
    "cat('\\nprecisio spam mitjana ',totalprecisio.spam/kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   221   39\n",
      "  spam     61  139\n",
      "preciso email  0.85\n",
      "precisio spam  0.695\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   244   49\n",
      "  spam     53  114\n",
      "preciso email  0.8327645\n",
      "precisio spam  0.6826347\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   221   69\n",
      "  spam     51  119\n",
      "preciso email  0.762069\n",
      "precisio spam  0.7\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   242   39\n",
      "  spam     64  115\n",
      "preciso email  0.86121\n",
      "precisio spam  0.6424581\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   214   56\n",
      "  spam     42  148\n",
      "preciso email  0.7925926\n",
      "precisio spam  0.7789474\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   242   52\n",
      "  spam     63  103\n",
      "preciso email  0.8231293\n",
      "precisio spam  0.6204819\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   227   47\n",
      "  spam     49  137\n",
      "preciso email  0.8284672\n",
      "precisio spam  0.7365591\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   233   50\n",
      "  spam     52  125\n",
      "preciso email  0.8233216\n",
      "precisio spam  0.7062147\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   207   42\n",
      "  spam     68  143\n",
      "preciso email  0.8313253\n",
      "precisio spam  0.6777251\n",
      "\n",
      "Matriu confusio\n",
      "       Predicted\n",
      "True    email spam\n",
      "  email   240   54\n",
      "  spam     50  116\n",
      "preciso email  0.8163265\n",
      "precisio spam  0.6987952\n",
      "\n",
      "preciso email mitjana  0.8221206\n",
      "precisio spam mitjana  0.6938816"
     ]
    }
   ],
   "source": [
    "k<-20\n",
    "\n",
    "kfold<-10\n",
    "totalprecisio.email<-0\n",
    "totalprecisio.spam<-0\n",
    "for (i in 1:kfold){\n",
    "    ITest<-((nrow(spam)/kfold)*(i-1)+1):((nrow(spam)/kfold)*i)\n",
    "    spam.train<-spam[-ITest,-ncol(spam)]\n",
    "    spam.test<-spam[ITest,-ncol(spam)]\n",
    "    spam.Ytrain<-spam[-ITest,ncol(spam)]\n",
    "    spam.Ytest<-spam[ITest,ncol(spam)]\n",
    "    spam.Yhat<-knn(spam.train, spam.test, spam.Ytrain, k = k)\n",
    "    C<-table(\"True\"=spam.Ytest,\"Predicted\"=spam.Yhat)\n",
    "    cat('\\nMatriu confusio\\n')\n",
    "    print(C)\n",
    "    precisio.email<-C[1]/(C[1]+C[3])\n",
    "    precisio.spam<-C[4]/(C[2]+C[4])\n",
    "    cat('preciso email ',+precisio.email)\n",
    "    cat('\\nprecisio spam ',precisio.spam)\n",
    "    cat('\\n')\n",
    "    totalprecisio.email<-totalprecisio.email + precisio.email\n",
    "    totalprecisio.spam<-totalprecisio.spam + precisio.spam\n",
    "}\n",
    "cat('\\npreciso email mitjana ',totalprecisio.email/kfold)\n",
    "cat('\\nprecisio spam mitjana ',totalprecisio.spam/kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veiem que optenim mes precisió en k = 10 que en k = 20. Encara que la precisió de predicció de spam és més baixa que amb email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresió logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x <- c(spam = 0, email = 1)\n",
    "spam.categorize<-spam[-nrow(spam)]\n",
    "spam.categorize$spam <- x[spam$spam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "ITest<-sample(1:nrow(spam),nrow(spam)/10,replace=FALSE)\n",
    "spam.train<-spam[-ITest,]\n",
    "spam.test<-spam[ITest,]\n",
    "spam.logist<-glm(spam~.,data=spam.train,family=binomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = spam ~ ., family = binomial, data = spam.train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-4.1312  -0.2143   0.0000   0.1044   5.1887  \n",
       "\n",
       "Coefficients:\n",
       "              Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept) -1.596e+00  1.486e-01 -10.737  < 2e-16 ***\n",
       "A.1         -5.423e-01  2.530e-01  -2.143 0.032111 *  \n",
       "A.2         -1.497e-01  7.534e-02  -1.987 0.046952 *  \n",
       "A.3          1.070e-01  1.145e-01   0.934 0.350057    \n",
       "A.4          2.833e+00  1.718e+00   1.649 0.099083 .  \n",
       "A.5          5.846e-01  1.075e-01   5.439 5.36e-08 ***\n",
       "A.6          8.709e-01  2.649e-01   3.288 0.001009 ** \n",
       "A.7          2.097e+00  3.354e-01   6.253 4.03e-10 ***\n",
       "A.8          4.752e-01  1.556e-01   3.053 0.002265 ** \n",
       "A.9          5.258e-01  2.847e-01   1.847 0.064750 .  \n",
       "A.10         1.212e-01  7.271e-02   1.668 0.095413 .  \n",
       "A.11        -1.250e-01  3.110e-01  -0.402 0.687814    \n",
       "A.12        -1.589e-01  7.861e-02  -2.021 0.043313 *  \n",
       "A.13        -2.991e-02  2.541e-01  -0.118 0.906282    \n",
       "A.14         1.391e-01  1.397e-01   0.995 0.319655    \n",
       "A.15         1.054e+00  7.130e-01   1.478 0.139478    \n",
       "A.16         1.124e+00  1.607e-01   6.992 2.72e-12 ***\n",
       "A.17         8.499e-01  2.209e-01   3.847 0.000119 ***\n",
       "A.18         1.296e-01  1.270e-01   1.020 0.307541    \n",
       "A.19         8.074e-02  3.726e-02   2.167 0.030271 *  \n",
       "A.20         9.576e-01  5.587e-01   1.714 0.086518 .  \n",
       "A.21         2.735e-01  5.751e-02   4.755 1.98e-06 ***\n",
       "A.22         1.888e-01  1.684e-01   1.121 0.262306    \n",
       "A.23         2.122e+00  4.805e-01   4.416 1.00e-05 ***\n",
       "A.24         3.697e-01  1.458e-01   2.535 0.011241 *  \n",
       "A.25        -1.864e+00  3.075e-01  -6.062 1.34e-09 ***\n",
       "A.26        -1.097e+00  4.546e-01  -2.414 0.015784 *  \n",
       "A.27        -1.086e+01  2.001e+00  -5.429 5.65e-08 ***\n",
       "A.28         4.534e-01  1.992e-01   2.277 0.022808 *  \n",
       "A.29        -4.866e+00  2.942e+00  -1.654 0.098140 .  \n",
       "A.30        -2.288e-01  3.230e-01  -0.708 0.478764    \n",
       "A.31        -1.636e-01  4.732e-01  -0.346 0.729562    \n",
       "A.32         4.273e+00  3.333e+00   1.282 0.199785    \n",
       "A.33        -7.407e-01  3.259e-01  -2.272 0.023064 *  \n",
       "A.34         1.476e+00  1.987e+00   0.743 0.457528    \n",
       "A.35        -1.793e+00  9.053e-01  -1.981 0.047606 *  \n",
       "A.36         8.604e-01  3.199e-01   2.689 0.007158 ** \n",
       "A.37        -3.117e-02  1.923e-01  -0.162 0.871229    \n",
       "A.38        -6.568e-01  4.499e-01  -1.460 0.144254    \n",
       "A.39        -8.206e-01  3.996e-01  -2.054 0.040012 *  \n",
       "A.40        -3.520e-01  3.711e-01  -0.949 0.342862    \n",
       "A.41        -4.754e+01  2.539e+01  -1.873 0.061128 .  \n",
       "A.42        -2.590e+00  8.733e-01  -2.966 0.003016 ** \n",
       "A.43        -1.250e+00  8.601e-01  -1.454 0.146016    \n",
       "A.44        -1.575e+00  5.971e-01  -2.637 0.008366 ** \n",
       "A.45        -8.420e-01  1.661e-01  -5.069 4.01e-07 ***\n",
       "A.46        -1.415e+00  2.779e-01  -5.094 3.51e-07 ***\n",
       "A.47        -2.162e+00  1.522e+00  -1.421 0.155431    \n",
       "A.48        -4.200e+00  1.657e+00  -2.535 0.011248 *  \n",
       "A.49        -1.304e+00  4.658e-01  -2.799 0.005129 ** \n",
       "A.50        -2.353e-01  2.631e-01  -0.894 0.371227    \n",
       "A.51        -6.470e-01  8.453e-01  -0.765 0.444048    \n",
       "A.52         3.369e-01  8.930e-02   3.772 0.000162 ***\n",
       "A.53         6.400e+00  8.054e-01   7.946 1.92e-15 ***\n",
       "A.54         2.979e+00  9.054e-01   3.290 0.001001 ** \n",
       "A.55         5.462e-03  1.815e-02   0.301 0.763478    \n",
       "A.56         9.053e-03  2.627e-03   3.446 0.000569 ***\n",
       "A.57         1.063e-03  2.383e-04   4.459 8.22e-06 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 5553.5  on 4140  degrees of freedom\n",
       "Residual deviance: 1626.8  on 4083  degrees of freedom\n",
       "AIC: 1742.8\n",
       "\n",
       "Number of Fisher Scoring iterations: 13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "       Predicted\n",
       "True      0   1\n",
       "  email 260  19\n",
       "  spam   20 161"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(spam.logist)\n",
    "spam.pred<-predict(spam.logist,newdata=spam.test, type=\"response\")\n",
    "spam.pred.crisp<-1*(spam.pred>=0.5)\n",
    "C<-table(\"True\"=spam.test$spam,\"Predicted\"=spam.pred.crisp)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  AIC=1742.84\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.11 + A.12 + A.13 + A.14 + A.15 + A.16 + A.17 + A.18 + \n",
      "    A.19 + A.20 + A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + \n",
      "    A.28 + A.29 + A.30 + A.31 + A.32 + A.33 + A.34 + A.35 + A.36 + \n",
      "    A.37 + A.38 + A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + A.45 + \n",
      "    A.46 + A.47 + A.48 + A.49 + A.50 + A.51 + A.52 + A.53 + A.54 + \n",
      "    A.55 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.13  1   1626.9 1740.9\n",
      "- A.37  1   1626.9 1740.9\n",
      "- A.55  1   1626.9 1740.9\n",
      "- A.11  1   1627.0 1741.0\n",
      "- A.31  1   1627.1 1741.1\n",
      "- A.30  1   1627.4 1741.4\n",
      "- A.34  1   1627.6 1741.6\n",
      "- A.3   1   1627.7 1741.7\n",
      "- A.50  1   1627.7 1741.7\n",
      "- A.14  1   1627.8 1741.8\n",
      "- A.51  1   1627.8 1741.8\n",
      "- A.40  1   1627.9 1741.9\n",
      "- A.18  1   1627.9 1741.9\n",
      "- A.32  1   1628.2 1742.2\n",
      "- A.22  1   1628.5 1742.5\n",
      "<none>      1626.8 1742.8\n",
      "- A.15  1   1629.5 1743.5\n",
      "- A.10  1   1629.8 1743.8\n",
      "- A.9   1   1630.4 1744.4\n",
      "- A.43  1   1630.9 1744.9\n",
      "- A.38  1   1631.0 1745.0\n",
      "- A.12  1   1631.3 1745.3\n",
      "- A.47  1   1631.4 1745.4\n",
      "- A.19  1   1631.4 1745.4\n",
      "- A.1   1   1632.0 1746.0\n",
      "- A.35  1   1632.8 1746.8\n",
      "- A.39  1   1632.8 1746.8\n",
      "- A.36  1   1633.8 1747.8\n",
      "- A.2   1   1634.4 1748.4\n",
      "- A.28  1   1634.4 1748.4\n",
      "- A.54  1   1635.0 1749.0\n",
      "- A.26  1   1635.1 1749.1\n",
      "- A.24  1   1636.8 1750.8\n",
      "- A.4   1   1637.0 1751.0\n",
      "- A.33  1   1637.0 1751.0\n",
      "- A.20  1   1637.2 1751.2\n",
      "- A.8   1   1639.2 1753.2\n",
      "- A.56  1   1639.9 1753.9\n",
      "- A.29  1   1640.2 1754.2\n",
      "- A.6   1   1640.6 1754.6\n",
      "- A.44  1   1642.2 1756.2\n",
      "- A.48  1   1642.4 1756.4\n",
      "- A.49  1   1642.9 1756.9\n",
      "- A.17  1   1645.0 1759.0\n",
      "- A.41  1   1648.6 1762.6\n",
      "- A.21  1   1650.7 1764.7\n",
      "- A.57  1   1651.0 1765.0\n",
      "- A.52  1   1655.2 1769.2\n",
      "- A.23  1   1661.8 1775.8\n",
      "- A.5   1   1665.1 1779.1\n",
      "- A.42  1   1671.2 1785.2\n",
      "- A.45  1   1674.8 1788.8\n",
      "- A.46  1   1684.8 1798.8\n",
      "- A.7   1   1708.1 1822.1\n",
      "- A.16  1   1713.2 1827.2\n",
      "- A.53  1   1727.9 1841.9\n",
      "- A.25  1   1734.6 1848.6\n",
      "- A.27  1   1851.4 1965.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1740.86\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.11 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + \n",
      "    A.20 + A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + \n",
      "    A.29 + A.30 + A.31 + A.32 + A.33 + A.34 + A.35 + A.36 + A.37 + \n",
      "    A.38 + A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + \n",
      "    A.47 + A.48 + A.49 + A.50 + A.51 + A.52 + A.53 + A.54 + A.55 + \n",
      "    A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.37  1   1626.9 1738.9\n",
      "- A.55  1   1627.0 1739.0\n",
      "- A.11  1   1627.0 1739.0\n",
      "- A.31  1   1627.1 1739.1\n",
      "- A.30  1   1627.4 1739.4\n",
      "- A.34  1   1627.6 1739.6\n",
      "- A.3   1   1627.7 1739.7\n",
      "- A.50  1   1627.7 1739.7\n",
      "- A.51  1   1627.8 1739.8\n",
      "- A.14  1   1627.8 1739.8\n",
      "- A.40  1   1627.9 1739.9\n",
      "- A.18  1   1627.9 1739.9\n",
      "- A.32  1   1628.3 1740.3\n",
      "- A.22  1   1628.5 1740.5\n",
      "<none>      1626.9 1740.9\n",
      "- A.15  1   1629.5 1741.5\n",
      "- A.10  1   1629.8 1741.8\n",
      "- A.9   1   1630.5 1742.5\n",
      "- A.43  1   1630.9 1742.9\n",
      "- A.38  1   1631.0 1743.0\n",
      "- A.12  1   1631.3 1743.3\n",
      "- A.47  1   1631.4 1743.4\n",
      "- A.19  1   1631.4 1743.4\n",
      "- A.1   1   1632.0 1744.0\n",
      "- A.35  1   1632.8 1744.8\n",
      "- A.39  1   1632.9 1744.9\n",
      "- A.36  1   1633.9 1745.9\n",
      "- A.2   1   1634.4 1746.4\n",
      "- A.28  1   1634.4 1746.4\n",
      "- A.54  1   1635.0 1747.0\n",
      "- A.26  1   1635.1 1747.1\n",
      "- A.24  1   1636.9 1748.9\n",
      "- A.33  1   1637.0 1749.0\n",
      "- A.4   1   1637.0 1749.0\n",
      "- A.20  1   1637.2 1749.2\n",
      "- A.8   1   1639.2 1751.2\n",
      "- A.56  1   1639.9 1751.9\n",
      "- A.29  1   1640.3 1752.3\n",
      "- A.6   1   1640.6 1752.6\n",
      "- A.44  1   1642.2 1754.2\n",
      "- A.48  1   1642.4 1754.4\n",
      "- A.49  1   1642.9 1754.9\n",
      "- A.17  1   1645.0 1757.0\n",
      "- A.41  1   1648.6 1760.6\n",
      "- A.21  1   1650.7 1762.7\n",
      "- A.57  1   1651.1 1763.1\n",
      "- A.52  1   1655.3 1767.3\n",
      "- A.23  1   1661.8 1773.8\n",
      "- A.5   1   1665.1 1777.1\n",
      "- A.42  1   1671.2 1783.2\n",
      "- A.45  1   1674.8 1786.8\n",
      "- A.46  1   1685.2 1797.2\n",
      "- A.7   1   1708.1 1820.1\n",
      "- A.16  1   1713.6 1825.6\n",
      "- A.53  1   1730.4 1842.4\n",
      "- A.25  1   1734.8 1846.8\n",
      "- A.27  1   1851.6 1963.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1738.88\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.11 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + \n",
      "    A.20 + A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + \n",
      "    A.29 + A.30 + A.31 + A.32 + A.33 + A.34 + A.35 + A.36 + A.38 + \n",
      "    A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + \n",
      "    A.48 + A.49 + A.50 + A.51 + A.52 + A.53 + A.54 + A.55 + A.56 + \n",
      "    A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.55  1   1627.0 1737.0\n",
      "- A.11  1   1627.0 1737.0\n",
      "- A.31  1   1627.1 1737.1\n",
      "- A.30  1   1627.4 1737.4\n",
      "- A.34  1   1627.6 1737.6\n",
      "- A.3   1   1627.7 1737.7\n",
      "- A.50  1   1627.8 1737.8\n",
      "- A.51  1   1627.8 1737.8\n",
      "- A.14  1   1627.8 1737.8\n",
      "- A.40  1   1627.9 1737.9\n",
      "- A.18  1   1627.9 1737.9\n",
      "- A.32  1   1628.3 1738.3\n",
      "- A.22  1   1628.6 1738.6\n",
      "<none>      1626.9 1738.9\n",
      "- A.15  1   1629.5 1739.5\n",
      "- A.10  1   1629.8 1739.8\n",
      "- A.9   1   1630.5 1740.5\n",
      "- A.43  1   1631.0 1741.0\n",
      "- A.38  1   1631.0 1741.0\n",
      "- A.12  1   1631.3 1741.3\n",
      "- A.47  1   1631.4 1741.4\n",
      "- A.19  1   1631.5 1741.5\n",
      "- A.1   1   1632.1 1742.1\n",
      "- A.35  1   1632.9 1742.9\n",
      "- A.39  1   1632.9 1742.9\n",
      "- A.36  1   1633.9 1743.9\n",
      "- A.2   1   1634.4 1744.4\n",
      "- A.28  1   1634.5 1744.5\n",
      "- A.54  1   1635.0 1745.0\n",
      "- A.26  1   1635.2 1745.2\n",
      "- A.24  1   1636.9 1746.9\n",
      "- A.4   1   1637.0 1747.0\n",
      "- A.33  1   1637.1 1747.1\n",
      "- A.20  1   1637.3 1747.3\n",
      "- A.8   1   1639.3 1749.3\n",
      "- A.56  1   1640.0 1750.0\n",
      "- A.29  1   1640.3 1750.3\n",
      "- A.6   1   1640.6 1750.6\n",
      "- A.44  1   1642.2 1752.2\n",
      "- A.48  1   1642.7 1752.7\n",
      "- A.49  1   1643.0 1753.0\n",
      "- A.17  1   1645.0 1755.0\n",
      "- A.41  1   1648.6 1758.6\n",
      "- A.21  1   1650.9 1760.9\n",
      "- A.57  1   1651.1 1761.1\n",
      "- A.52  1   1655.3 1765.3\n",
      "- A.23  1   1661.9 1771.9\n",
      "- A.5   1   1665.1 1775.1\n",
      "- A.42  1   1671.2 1781.2\n",
      "- A.45  1   1674.8 1784.8\n",
      "- A.46  1   1686.2 1796.2\n",
      "- A.7   1   1708.3 1818.3\n",
      "- A.16  1   1713.8 1823.8\n",
      "- A.53  1   1730.4 1840.4\n",
      "- A.25  1   1735.0 1845.0\n",
      "- A.27  1   1851.7 1961.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1736.97\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.11 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + \n",
      "    A.20 + A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + \n",
      "    A.29 + A.30 + A.31 + A.32 + A.33 + A.34 + A.35 + A.36 + A.38 + \n",
      "    A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + \n",
      "    A.48 + A.49 + A.50 + A.51 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance   AIC\n",
      "- A.11  1     1627  1735\n",
      "- A.31  1     1627  1735\n",
      "- A.30  1     1627  1735\n",
      "- A.34  1     1628  1736\n",
      "- A.3   1     1628  1736\n",
      "- A.50  1     1628  1736\n",
      "- A.14  1     1628  1736\n",
      "- A.51  1     1628  1736\n",
      "- A.18  1     1628  1736\n",
      "- A.32  1     1628  1736\n",
      "- A.22  1     1629  1737\n",
      "<none>        1627  1737\n",
      "- A.15  1     1630  1738\n",
      "- A.10  1     1630  1738\n",
      "- A.9   1     1631  1739\n",
      "- A.43  1     1631  1739\n",
      "- A.38  1     1631  1739\n",
      "- A.47  1     1631  1739\n",
      "- A.12  1     1631  1739\n",
      "- A.19  1     1632  1740\n",
      "- A.1   1     1632  1740\n",
      "- A.39  1     1633  1741\n",
      "- A.35  1     1633  1741\n",
      "- A.36  1     1634  1742\n",
      "- A.2   1     1635  1743\n",
      "- A.28  1     1635  1743\n",
      "- A.54  1     1635  1743\n",
      "- A.26  1     1635  1743\n",
      "- A.24  1     1637  1745\n",
      "- A.4   1     1637  1745\n",
      "- A.33  1     1637  1745\n",
      "- A.20  1     1637  1745\n",
      "- A.8   1     1639  1747\n",
      "- A.29  1     1640  1748\n",
      "- A.6   1     1641  1749\n",
      "- A.44  1     1642  1750\n",
      "- A.48  1     1643  1751\n",
      "- A.49  1     1643  1751\n",
      "- A.17  1     1645  1753\n",
      "- A.41  1     1649  1757\n",
      "- A.21  1     1651  1759\n",
      "- A.57  1     1652  1760\n",
      "- A.52  1     1656  1764\n",
      "- A.56  1     1659  1767\n",
      "- A.23  1     1662  1770\n",
      "- A.5   1     1665  1773\n",
      "- A.42  1     1672  1780\n",
      "- A.45  1     1675  1783\n",
      "- A.46  1     1686  1794\n",
      "- A.7   1     1709  1817\n",
      "- A.16  1     1714  1822\n",
      "- A.53  1     1731  1839\n",
      "- A.25  1     1735  1843\n",
      "- A.27  1     1853  1961\n",
      "- A.40  1    37630 37738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1735.15\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + \n",
      "    A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + \n",
      "    A.30 + A.31 + A.32 + A.33 + A.34 + A.35 + A.36 + A.38 + A.39 + \n",
      "    A.40 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + \n",
      "    A.49 + A.50 + A.51 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: algorithm did not converge\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance   AIC\n",
      "- A.31  1     1627  1733\n",
      "- A.30  1     1628  1734\n",
      "- A.34  1     1628  1734\n",
      "- A.50  1     1628  1734\n",
      "- A.14  1     1628  1734\n",
      "- A.3   1     1628  1734\n",
      "- A.51  1     1628  1734\n",
      "- A.18  1     1628  1734\n",
      "- A.32  1     1629  1735\n",
      "- A.22  1     1629  1735\n",
      "<none>        1627  1735\n",
      "- A.15  1     1630  1736\n",
      "- A.10  1     1630  1736\n",
      "- A.9   1     1631  1737\n",
      "- A.43  1     1631  1737\n",
      "- A.38  1     1631  1737\n",
      "- A.47  1     1632  1738\n",
      "- A.19  1     1632  1738\n",
      "- A.12  1     1632  1738\n",
      "- A.39  1     1633  1739\n",
      "- A.35  1     1633  1739\n",
      "- A.1   1     1634  1740\n",
      "- A.36  1     1634  1740\n",
      "- A.2   1     1635  1741\n",
      "- A.28  1     1635  1741\n",
      "- A.26  1     1635  1741\n",
      "- A.54  1     1635  1741\n",
      "- A.4   1     1637  1743\n",
      "- A.20  1     1637  1743\n",
      "- A.24  1     1637  1743\n",
      "- A.33  1     1637  1743\n",
      "- A.8   1     1639  1745\n",
      "- A.29  1     1640  1746\n",
      "- A.6   1     1641  1747\n",
      "- A.44  1     1642  1748\n",
      "- A.48  1     1643  1749\n",
      "- A.49  1     1643  1749\n",
      "- A.17  1     1645  1751\n",
      "- A.41  1     1649  1755\n",
      "- A.21  1     1652  1758\n",
      "- A.57  1     1652  1758\n",
      "- A.52  1     1656  1762\n",
      "- A.56  1     1659  1765\n",
      "- A.23  1     1662  1768\n",
      "- A.5   1     1665  1771\n",
      "- A.42  1     1672  1778\n",
      "- A.45  1     1675  1781\n",
      "- A.46  1     1687  1793\n",
      "- A.7   1     1709  1815\n",
      "- A.16  1     1714  1820\n",
      "- A.53  1     1732  1838\n",
      "- A.25  1     1735  1841\n",
      "- A.27  1     1853  1959\n",
      "- A.40  1    34602 34708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1733.39\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + \n",
      "    A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + \n",
      "    A.30 + A.32 + A.33 + A.34 + A.35 + A.36 + A.38 + A.39 + A.40 + \n",
      "    A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + A.49 + \n",
      "    A.50 + A.51 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.30  1   1627.9 1731.9\n",
      "- A.34  1   1628.2 1732.2\n",
      "- A.50  1   1628.2 1732.2\n",
      "- A.14  1   1628.3 1732.3\n",
      "- A.3   1   1628.3 1732.3\n",
      "- A.51  1   1628.3 1732.3\n",
      "- A.18  1   1628.4 1732.4\n",
      "- A.40  1   1628.4 1732.4\n",
      "- A.32  1   1628.8 1732.8\n",
      "- A.22  1   1629.1 1733.1\n",
      "<none>      1627.4 1733.4\n",
      "- A.15  1   1630.0 1734.0\n",
      "- A.10  1   1630.3 1734.3\n",
      "- A.9   1   1631.0 1735.0\n",
      "- A.43  1   1631.5 1735.5\n",
      "- A.38  1   1631.6 1735.6\n",
      "- A.47  1   1631.8 1735.8\n",
      "- A.19  1   1632.0 1736.0\n",
      "- A.12  1   1632.1 1736.1\n",
      "- A.39  1   1633.4 1737.4\n",
      "- A.35  1   1633.7 1737.7\n",
      "- A.1   1   1634.2 1738.2\n",
      "- A.36  1   1634.4 1738.4\n",
      "- A.2   1   1634.9 1738.9\n",
      "- A.28  1   1635.0 1739.0\n",
      "- A.26  1   1635.5 1739.5\n",
      "- A.54  1   1635.6 1739.6\n",
      "- A.4   1   1637.6 1741.6\n",
      "- A.20  1   1637.6 1741.6\n",
      "- A.24  1   1637.6 1741.6\n",
      "- A.33  1   1637.7 1741.7\n",
      "- A.8   1   1639.7 1743.7\n",
      "- A.29  1   1640.6 1744.6\n",
      "- A.6   1   1641.1 1745.1\n",
      "- A.48  1   1643.1 1747.1\n",
      "- A.49  1   1643.5 1747.5\n",
      "- A.44  1   1645.4 1749.4\n",
      "- A.17  1   1645.8 1749.8\n",
      "- A.41  1   1649.0 1753.0\n",
      "- A.21  1   1652.5 1756.5\n",
      "- A.57  1   1652.7 1756.7\n",
      "- A.52  1   1656.0 1760.0\n",
      "- A.56  1   1658.9 1762.9\n",
      "- A.23  1   1662.2 1766.2\n",
      "- A.5   1   1665.7 1769.7\n",
      "- A.42  1   1672.0 1776.0\n",
      "- A.45  1   1675.2 1779.2\n",
      "- A.46  1   1686.7 1790.7\n",
      "- A.7   1   1709.0 1813.0\n",
      "- A.16  1   1714.5 1818.5\n",
      "- A.53  1   1732.0 1836.0\n",
      "- A.25  1   1738.1 1842.1\n",
      "- A.27  1   1856.2 1960.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1731.93\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + \n",
      "    A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + \n",
      "    A.32 + A.33 + A.34 + A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + \n",
      "    A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + A.49 + A.50 + \n",
      "    A.51 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.34  1   1628.7 1730.7\n",
      "- A.50  1   1628.8 1730.8\n",
      "- A.3   1   1628.8 1730.8\n",
      "- A.14  1   1628.8 1730.8\n",
      "- A.51  1   1628.8 1730.8\n",
      "- A.40  1   1628.9 1730.9\n",
      "- A.18  1   1629.0 1731.0\n",
      "- A.32  1   1629.3 1731.3\n",
      "- A.22  1   1629.7 1731.7\n",
      "<none>      1627.9 1731.9\n",
      "- A.15  1   1630.5 1732.5\n",
      "- A.10  1   1630.8 1732.8\n",
      "- A.9   1   1631.4 1733.4\n",
      "- A.43  1   1632.0 1734.0\n",
      "- A.38  1   1632.1 1734.1\n",
      "- A.47  1   1632.3 1734.3\n",
      "- A.12  1   1632.5 1734.5\n",
      "- A.19  1   1632.5 1734.5\n",
      "- A.39  1   1634.0 1736.0\n",
      "- A.35  1   1634.2 1736.2\n",
      "- A.1   1   1634.7 1736.7\n",
      "- A.36  1   1635.0 1737.0\n",
      "- A.2   1   1635.4 1737.4\n",
      "- A.28  1   1635.5 1737.5\n",
      "- A.26  1   1636.1 1738.1\n",
      "- A.54  1   1636.2 1738.2\n",
      "- A.33  1   1638.1 1740.1\n",
      "- A.4   1   1638.1 1740.1\n",
      "- A.24  1   1638.2 1740.2\n",
      "- A.20  1   1638.3 1740.3\n",
      "- A.8   1   1640.3 1742.3\n",
      "- A.29  1   1641.3 1743.3\n",
      "- A.6   1   1641.4 1743.4\n",
      "- A.48  1   1643.7 1745.7\n",
      "- A.49  1   1644.0 1746.0\n",
      "- A.44  1   1646.1 1748.1\n",
      "- A.17  1   1646.3 1748.3\n",
      "- A.41  1   1649.7 1751.7\n",
      "- A.21  1   1653.3 1755.3\n",
      "- A.57  1   1653.7 1755.7\n",
      "- A.52  1   1656.8 1758.8\n",
      "- A.56  1   1659.3 1761.3\n",
      "- A.23  1   1663.0 1765.0\n",
      "- A.5   1   1666.3 1768.3\n",
      "- A.42  1   1672.3 1774.3\n",
      "- A.45  1   1675.5 1777.5\n",
      "- A.46  1   1687.0 1789.0\n",
      "- A.7   1   1709.6 1811.6\n",
      "- A.16  1   1715.1 1817.1\n",
      "- A.53  1   1732.8 1834.8\n",
      "- A.25  1   1744.2 1846.2\n",
      "- A.27  1   1856.3 1958.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1730.69\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + \n",
      "    A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + \n",
      "    A.32 + A.33 + A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + A.42 + \n",
      "    A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + A.49 + A.50 + A.51 + \n",
      "    A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.50  1   1629.5 1729.5\n",
      "- A.3   1   1629.6 1729.6\n",
      "- A.14  1   1629.6 1729.6\n",
      "- A.51  1   1629.6 1729.6\n",
      "- A.40  1   1629.7 1729.7\n",
      "- A.18  1   1629.7 1729.7\n",
      "- A.32  1   1630.3 1730.3\n",
      "- A.22  1   1630.5 1730.5\n",
      "<none>      1628.7 1730.7\n",
      "- A.15  1   1631.2 1731.2\n",
      "- A.10  1   1631.7 1731.7\n",
      "- A.9   1   1632.1 1732.1\n",
      "- A.43  1   1632.8 1732.8\n",
      "- A.38  1   1632.8 1732.8\n",
      "- A.47  1   1633.1 1733.1\n",
      "- A.19  1   1633.2 1733.2\n",
      "- A.12  1   1633.4 1733.4\n",
      "- A.39  1   1634.7 1734.7\n",
      "- A.35  1   1635.0 1735.0\n",
      "- A.1   1   1635.3 1735.3\n",
      "- A.36  1   1635.8 1735.8\n",
      "- A.2   1   1636.2 1736.2\n",
      "- A.28  1   1636.3 1736.3\n",
      "- A.26  1   1636.8 1736.8\n",
      "- A.54  1   1636.9 1736.9\n",
      "- A.33  1   1638.9 1738.9\n",
      "- A.4   1   1638.9 1738.9\n",
      "- A.24  1   1639.0 1739.0\n",
      "- A.20  1   1639.0 1739.0\n",
      "- A.8   1   1641.1 1741.1\n",
      "- A.29  1   1642.1 1742.1\n",
      "- A.6   1   1642.1 1742.1\n",
      "- A.48  1   1644.4 1744.4\n",
      "- A.49  1   1644.8 1744.8\n",
      "- A.44  1   1646.9 1746.9\n",
      "- A.17  1   1647.0 1747.0\n",
      "- A.41  1   1650.5 1750.5\n",
      "- A.21  1   1654.1 1754.1\n",
      "- A.57  1   1654.7 1754.7\n",
      "- A.52  1   1658.2 1758.2\n",
      "- A.56  1   1659.9 1759.9\n",
      "- A.23  1   1663.7 1763.7\n",
      "- A.5   1   1667.0 1767.0\n",
      "- A.42  1   1673.1 1773.1\n",
      "- A.45  1   1676.4 1776.4\n",
      "- A.46  1   1687.9 1787.9\n",
      "- A.7   1   1710.3 1810.3\n",
      "- A.16  1   1715.5 1815.5\n",
      "- A.53  1   1733.5 1833.5\n",
      "- A.25  1   1744.9 1844.9\n",
      "- A.27  1   1856.4 1956.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1729.55\n",
      "spam ~ A.1 + A.2 + A.3 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + \n",
      "    A.10 + A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + \n",
      "    A.21 + A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + \n",
      "    A.32 + A.33 + A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + A.42 + \n",
      "    A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + A.49 + A.51 + A.52 + \n",
      "    A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.3   1   1630.4 1728.4\n",
      "- A.51  1   1630.5 1728.5\n",
      "- A.14  1   1630.5 1728.5\n",
      "- A.40  1   1630.5 1728.5\n",
      "- A.18  1   1630.6 1728.6\n",
      "- A.32  1   1631.1 1729.1\n",
      "- A.22  1   1631.5 1729.5\n",
      "<none>      1629.5 1729.5\n",
      "- A.15  1   1632.1 1730.1\n",
      "- A.10  1   1632.6 1730.6\n",
      "- A.9   1   1632.9 1730.9\n",
      "- A.43  1   1633.6 1731.6\n",
      "- A.38  1   1633.6 1731.6\n",
      "- A.47  1   1634.0 1732.0\n",
      "- A.12  1   1634.1 1732.1\n",
      "- A.19  1   1634.5 1732.5\n",
      "- A.39  1   1635.5 1733.5\n",
      "- A.35  1   1635.8 1733.8\n",
      "- A.1   1   1636.1 1734.1\n",
      "- A.28  1   1636.3 1734.3\n",
      "- A.36  1   1636.7 1734.7\n",
      "- A.2   1   1636.8 1734.8\n",
      "- A.26  1   1637.7 1735.7\n",
      "- A.54  1   1637.7 1735.7\n",
      "- A.33  1   1639.7 1737.7\n",
      "- A.24  1   1639.7 1737.7\n",
      "- A.20  1   1639.8 1737.8\n",
      "- A.4   1   1639.8 1737.8\n",
      "- A.8   1   1642.2 1740.2\n",
      "- A.6   1   1642.8 1740.8\n",
      "- A.29  1   1643.0 1741.0\n",
      "- A.48  1   1645.3 1743.3\n",
      "- A.49  1   1645.7 1743.7\n",
      "- A.44  1   1647.6 1745.6\n",
      "- A.17  1   1647.8 1745.8\n",
      "- A.41  1   1651.3 1749.3\n",
      "- A.57  1   1655.3 1753.3\n",
      "- A.21  1   1655.4 1753.4\n",
      "- A.52  1   1659.2 1757.2\n",
      "- A.56  1   1661.1 1759.1\n",
      "- A.23  1   1664.3 1762.3\n",
      "- A.5   1   1668.3 1766.3\n",
      "- A.42  1   1674.0 1772.0\n",
      "- A.45  1   1677.5 1775.5\n",
      "- A.46  1   1688.5 1786.5\n",
      "- A.7   1   1711.4 1809.4\n",
      "- A.16  1   1716.5 1814.5\n",
      "- A.53  1   1734.2 1832.2\n",
      "- A.25  1   1745.2 1843.2\n",
      "- A.27  1   1856.8 1954.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1728.42\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.14 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + A.21 + \n",
      "    A.22 + A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.32 + \n",
      "    A.33 + A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + A.42 + A.43 + \n",
      "    A.44 + A.45 + A.46 + A.47 + A.48 + A.49 + A.51 + A.52 + A.53 + \n",
      "    A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.14  1   1631.3 1727.3\n",
      "- A.51  1   1631.4 1727.4\n",
      "- A.40  1   1631.4 1727.4\n",
      "- A.18  1   1631.6 1727.6\n",
      "- A.32  1   1632.1 1728.1\n",
      "- A.22  1   1632.3 1728.3\n",
      "<none>      1630.4 1728.4\n",
      "- A.15  1   1633.1 1729.1\n",
      "- A.10  1   1633.3 1729.3\n",
      "- A.9   1   1633.9 1729.9\n",
      "- A.43  1   1634.5 1730.5\n",
      "- A.38  1   1634.6 1730.6\n",
      "- A.47  1   1634.6 1730.6\n",
      "- A.12  1   1634.9 1730.9\n",
      "- A.19  1   1635.8 1731.8\n",
      "- A.39  1   1636.3 1732.3\n",
      "- A.35  1   1636.8 1732.8\n",
      "- A.28  1   1637.1 1733.1\n",
      "- A.1   1   1637.1 1733.1\n",
      "- A.36  1   1637.4 1733.4\n",
      "- A.2   1   1637.8 1733.8\n",
      "- A.54  1   1638.6 1734.6\n",
      "- A.26  1   1638.6 1734.6\n",
      "- A.24  1   1640.5 1736.5\n",
      "- A.4   1   1640.6 1736.6\n",
      "- A.20  1   1640.7 1736.7\n",
      "- A.33  1   1640.8 1736.8\n",
      "- A.8   1   1642.8 1738.8\n",
      "- A.29  1   1643.5 1739.5\n",
      "- A.6   1   1643.5 1739.5\n",
      "- A.48  1   1646.3 1742.3\n",
      "- A.49  1   1646.7 1742.7\n",
      "- A.17  1   1648.7 1744.7\n",
      "- A.44  1   1648.8 1744.8\n",
      "- A.41  1   1652.3 1748.3\n",
      "- A.57  1   1656.2 1752.2\n",
      "- A.21  1   1656.6 1752.6\n",
      "- A.52  1   1660.1 1756.1\n",
      "- A.56  1   1662.6 1758.6\n",
      "- A.23  1   1665.1 1761.1\n",
      "- A.5   1   1669.0 1765.0\n",
      "- A.42  1   1674.8 1770.8\n",
      "- A.45  1   1678.8 1774.8\n",
      "- A.46  1   1690.6 1786.6\n",
      "- A.7   1   1711.7 1807.7\n",
      "- A.16  1   1718.0 1814.0\n",
      "- A.53  1   1735.2 1831.2\n",
      "- A.25  1   1747.6 1843.6\n",
      "- A.27  1   1859.5 1955.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1727.26\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + A.21 + A.22 + \n",
      "    A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.32 + A.33 + \n",
      "    A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + \n",
      "    A.45 + A.46 + A.47 + A.48 + A.49 + A.51 + A.52 + A.53 + A.54 + \n",
      "    A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.51  1   1632.2 1726.2\n",
      "- A.40  1   1632.3 1726.3\n",
      "- A.18  1   1632.4 1726.4\n",
      "- A.32  1   1632.9 1726.9\n",
      "- A.22  1   1633.1 1727.1\n",
      "<none>      1631.3 1727.3\n",
      "- A.15  1   1634.0 1728.0\n",
      "- A.10  1   1634.2 1728.2\n",
      "- A.9   1   1634.7 1728.7\n",
      "- A.43  1   1635.3 1729.3\n",
      "- A.38  1   1635.4 1729.4\n",
      "- A.47  1   1635.5 1729.5\n",
      "- A.12  1   1635.8 1729.8\n",
      "- A.19  1   1636.6 1730.6\n",
      "- A.39  1   1637.3 1731.3\n",
      "- A.35  1   1637.6 1731.6\n",
      "- A.28  1   1637.9 1731.9\n",
      "- A.1   1   1638.0 1732.0\n",
      "- A.36  1   1638.8 1732.8\n",
      "- A.2   1   1638.8 1732.8\n",
      "- A.54  1   1639.4 1733.4\n",
      "- A.26  1   1639.7 1733.7\n",
      "- A.24  1   1641.2 1735.2\n",
      "- A.4   1   1641.4 1735.4\n",
      "- A.20  1   1641.6 1735.6\n",
      "- A.33  1   1641.7 1735.7\n",
      "- A.8   1   1643.6 1737.6\n",
      "- A.29  1   1644.2 1738.2\n",
      "- A.6   1   1644.2 1738.2\n",
      "- A.48  1   1647.3 1741.3\n",
      "- A.49  1   1647.6 1741.6\n",
      "- A.17  1   1649.3 1743.3\n",
      "- A.44  1   1649.6 1743.6\n",
      "- A.41  1   1653.2 1747.2\n",
      "- A.57  1   1657.1 1751.1\n",
      "- A.21  1   1657.2 1751.2\n",
      "- A.52  1   1660.7 1754.7\n",
      "- A.56  1   1663.8 1757.8\n",
      "- A.23  1   1665.9 1759.9\n",
      "- A.5   1   1669.8 1763.8\n",
      "- A.42  1   1674.8 1768.8\n",
      "- A.45  1   1679.6 1773.6\n",
      "- A.46  1   1691.7 1785.7\n",
      "- A.7   1   1712.2 1806.2\n",
      "- A.16  1   1718.8 1812.8\n",
      "- A.53  1   1737.6 1831.6\n",
      "- A.25  1   1749.0 1843.0\n",
      "- A.27  1   1859.6 1953.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1726.25\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + A.21 + A.22 + \n",
      "    A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.32 + A.33 + \n",
      "    A.35 + A.36 + A.38 + A.39 + A.40 + A.41 + A.42 + A.43 + A.44 + \n",
      "    A.45 + A.46 + A.47 + A.48 + A.49 + A.52 + A.53 + A.54 + A.56 + \n",
      "    A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.40  1   1633.2 1725.2\n",
      "- A.18  1   1633.4 1725.4\n",
      "- A.32  1   1633.9 1725.9\n",
      "- A.22  1   1634.1 1726.1\n",
      "<none>      1632.2 1726.2\n",
      "- A.15  1   1635.0 1727.0\n",
      "- A.10  1   1635.2 1727.2\n",
      "- A.9   1   1635.7 1727.7\n",
      "- A.43  1   1636.3 1728.3\n",
      "- A.38  1   1636.4 1728.4\n",
      "- A.47  1   1636.4 1728.4\n",
      "- A.12  1   1636.7 1728.7\n",
      "- A.19  1   1637.8 1729.8\n",
      "- A.39  1   1638.2 1730.2\n",
      "- A.35  1   1638.5 1730.5\n",
      "- A.28  1   1638.9 1730.9\n",
      "- A.1   1   1639.0 1731.0\n",
      "- A.2   1   1639.7 1731.7\n",
      "- A.36  1   1639.9 1731.9\n",
      "- A.54  1   1640.4 1732.4\n",
      "- A.26  1   1640.7 1732.7\n",
      "- A.24  1   1642.2 1734.2\n",
      "- A.4   1   1642.5 1734.5\n",
      "- A.20  1   1642.6 1734.6\n",
      "- A.33  1   1642.8 1734.8\n",
      "- A.8   1   1644.7 1736.7\n",
      "- A.6   1   1645.1 1737.1\n",
      "- A.29  1   1645.2 1737.2\n",
      "- A.48  1   1648.2 1740.2\n",
      "- A.49  1   1648.5 1740.5\n",
      "- A.17  1   1650.5 1742.5\n",
      "- A.44  1   1650.5 1742.5\n",
      "- A.41  1   1654.2 1746.2\n",
      "- A.57  1   1657.8 1749.8\n",
      "- A.21  1   1658.2 1750.2\n",
      "- A.52  1   1661.9 1753.9\n",
      "- A.56  1   1665.1 1757.1\n",
      "- A.23  1   1666.5 1758.5\n",
      "- A.5   1   1671.3 1763.3\n",
      "- A.42  1   1675.8 1767.8\n",
      "- A.45  1   1680.7 1772.7\n",
      "- A.46  1   1692.4 1784.4\n",
      "- A.7   1   1713.2 1805.2\n",
      "- A.16  1   1719.9 1811.9\n",
      "- A.53  1   1739.4 1831.4\n",
      "- A.25  1   1749.7 1841.7\n",
      "- A.27  1   1861.4 1953.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1725.25\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.18 + A.19 + A.20 + A.21 + A.22 + \n",
      "    A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.32 + A.33 + \n",
      "    A.35 + A.36 + A.38 + A.39 + A.41 + A.42 + A.43 + A.44 + A.45 + \n",
      "    A.46 + A.47 + A.48 + A.49 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.18  1   1634.4 1724.4\n",
      "- A.32  1   1634.9 1724.9\n",
      "<none>      1633.2 1725.2\n",
      "- A.22  1   1635.3 1725.3\n",
      "- A.15  1   1635.9 1725.9\n",
      "- A.10  1   1636.0 1726.0\n",
      "- A.9   1   1636.5 1726.5\n",
      "- A.43  1   1637.3 1727.3\n",
      "- A.38  1   1637.3 1727.3\n",
      "- A.47  1   1637.5 1727.5\n",
      "- A.12  1   1637.5 1727.5\n",
      "- A.19  1   1639.0 1729.0\n",
      "- A.39  1   1639.2 1729.2\n",
      "- A.35  1   1639.5 1729.5\n",
      "- A.1   1   1639.9 1729.9\n",
      "- A.28  1   1639.9 1729.9\n",
      "- A.2   1   1640.6 1730.6\n",
      "- A.54  1   1640.7 1730.7\n",
      "- A.36  1   1640.8 1730.8\n",
      "- A.26  1   1641.7 1731.7\n",
      "- A.24  1   1643.3 1733.3\n",
      "- A.4   1   1643.5 1733.5\n",
      "- A.20  1   1643.6 1733.6\n",
      "- A.33  1   1643.7 1733.7\n",
      "- A.8   1   1645.7 1735.7\n",
      "- A.29  1   1646.1 1736.1\n",
      "- A.6   1   1646.1 1736.1\n",
      "- A.48  1   1649.3 1739.3\n",
      "- A.49  1   1649.5 1739.5\n",
      "- A.44  1   1651.1 1741.1\n",
      "- A.17  1   1651.2 1741.2\n",
      "- A.41  1   1655.1 1745.1\n",
      "- A.57  1   1658.9 1748.9\n",
      "- A.21  1   1658.9 1748.9\n",
      "- A.52  1   1663.0 1753.0\n",
      "- A.56  1   1666.2 1756.2\n",
      "- A.23  1   1667.4 1757.4\n",
      "- A.5   1   1672.5 1762.5\n",
      "- A.42  1   1676.3 1766.3\n",
      "- A.45  1   1681.5 1771.5\n",
      "- A.46  1   1693.0 1783.0\n",
      "- A.7   1   1713.8 1803.8\n",
      "- A.16  1   1720.9 1810.9\n",
      "- A.53  1   1740.8 1830.8\n",
      "- A.25  1   1749.8 1839.8\n",
      "- A.27  1   1870.5 1960.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1724.36\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.19 + A.20 + A.21 + A.22 + A.23 + \n",
      "    A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.32 + A.33 + A.35 + \n",
      "    A.36 + A.38 + A.39 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + \n",
      "    A.47 + A.48 + A.49 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.32  1   1636.0 1724.0\n",
      "<none>      1634.4 1724.4\n",
      "- A.22  1   1636.4 1724.4\n",
      "- A.10  1   1637.1 1725.1\n",
      "- A.15  1   1637.5 1725.5\n",
      "- A.9   1   1637.6 1725.6\n",
      "- A.47  1   1638.3 1726.3\n",
      "- A.43  1   1638.4 1726.4\n",
      "- A.38  1   1638.6 1726.6\n",
      "- A.12  1   1638.8 1726.8\n",
      "- A.39  1   1640.5 1728.5\n",
      "- A.19  1   1640.6 1728.6\n",
      "- A.35  1   1640.8 1728.8\n",
      "- A.28  1   1641.1 1729.1\n",
      "- A.1   1   1641.4 1729.4\n",
      "- A.2   1   1641.5 1729.5\n",
      "- A.54  1   1641.6 1729.6\n",
      "- A.36  1   1641.8 1729.8\n",
      "- A.26  1   1642.8 1730.8\n",
      "- A.24  1   1644.2 1732.2\n",
      "- A.4   1   1644.5 1732.5\n",
      "- A.20  1   1644.6 1732.6\n",
      "- A.33  1   1645.0 1733.0\n",
      "- A.8   1   1646.8 1734.8\n",
      "- A.6   1   1647.0 1735.0\n",
      "- A.29  1   1647.4 1735.4\n",
      "- A.48  1   1650.5 1738.5\n",
      "- A.49  1   1650.8 1738.8\n",
      "- A.44  1   1652.4 1740.4\n",
      "- A.17  1   1652.8 1740.8\n",
      "- A.41  1   1656.1 1744.1\n",
      "- A.57  1   1659.7 1747.7\n",
      "- A.21  1   1660.5 1748.5\n",
      "- A.52  1   1663.9 1751.9\n",
      "- A.23  1   1668.1 1756.1\n",
      "- A.56  1   1668.2 1756.2\n",
      "- A.5   1   1673.6 1761.6\n",
      "- A.42  1   1677.9 1765.9\n",
      "- A.45  1   1684.2 1772.2\n",
      "- A.46  1   1695.1 1783.1\n",
      "- A.7   1   1718.7 1806.7\n",
      "- A.16  1   1725.0 1813.0\n",
      "- A.53  1   1744.3 1832.3\n",
      "- A.25  1   1751.7 1839.7\n",
      "- A.27  1   1876.9 1964.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1724.02\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.19 + A.20 + A.21 + A.22 + A.23 + \n",
      "    A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.33 + A.35 + A.36 + \n",
      "    A.38 + A.39 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + \n",
      "    A.48 + A.49 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "- A.22  1   1638.0 1724.0\n",
      "<none>      1636.0 1724.0\n",
      "- A.10  1   1638.7 1724.7\n",
      "- A.15  1   1639.1 1725.1\n",
      "- A.9   1   1639.1 1725.1\n",
      "- A.47  1   1640.0 1726.0\n",
      "- A.43  1   1640.1 1726.1\n",
      "- A.38  1   1640.3 1726.3\n",
      "- A.12  1   1640.6 1726.6\n",
      "- A.19  1   1642.0 1728.0\n",
      "- A.39  1   1642.2 1728.2\n",
      "- A.35  1   1642.4 1728.4\n",
      "- A.28  1   1642.8 1728.8\n",
      "- A.2   1   1643.1 1729.1\n",
      "- A.1   1   1643.2 1729.2\n",
      "- A.54  1   1643.2 1729.2\n",
      "- A.36  1   1643.5 1729.5\n",
      "- A.26  1   1644.5 1730.5\n",
      "- A.24  1   1646.1 1732.1\n",
      "- A.4   1   1646.1 1732.1\n",
      "- A.33  1   1646.7 1732.7\n",
      "- A.20  1   1647.0 1733.0\n",
      "- A.8   1   1648.4 1734.4\n",
      "- A.6   1   1648.6 1734.6\n",
      "- A.29  1   1649.1 1735.1\n",
      "- A.48  1   1652.2 1738.2\n",
      "- A.49  1   1652.2 1738.2\n",
      "- A.17  1   1654.2 1740.2\n",
      "- A.44  1   1654.5 1740.5\n",
      "- A.41  1   1657.9 1743.9\n",
      "- A.57  1   1661.8 1747.8\n",
      "- A.21  1   1662.8 1748.8\n",
      "- A.52  1   1665.6 1751.6\n",
      "- A.56  1   1669.2 1755.2\n",
      "- A.23  1   1669.7 1755.7\n",
      "- A.5   1   1674.9 1760.9\n",
      "- A.42  1   1679.5 1765.5\n",
      "- A.45  1   1686.0 1772.0\n",
      "- A.46  1   1696.9 1782.9\n",
      "- A.7   1   1720.1 1806.1\n",
      "- A.16  1   1726.2 1812.2\n",
      "- A.53  1   1745.3 1831.3\n",
      "- A.25  1   1753.9 1839.9\n",
      "- A.27  1   1880.7 1966.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step:  AIC=1723.97\n",
      "spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + A.9 + A.10 + \n",
      "    A.12 + A.15 + A.16 + A.17 + A.19 + A.20 + A.21 + A.23 + A.24 + \n",
      "    A.25 + A.26 + A.27 + A.28 + A.29 + A.33 + A.35 + A.36 + A.38 + \n",
      "    A.39 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + A.47 + A.48 + \n",
      "    A.49 + A.52 + A.53 + A.54 + A.56 + A.57\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Df Deviance    AIC\n",
      "<none>      1638.0 1724.0\n",
      "- A.10  1   1640.6 1724.6\n",
      "- A.15  1   1640.8 1724.8\n",
      "- A.9   1   1640.8 1724.8\n",
      "- A.47  1   1642.1 1726.1\n",
      "- A.43  1   1642.1 1726.1\n",
      "- A.38  1   1642.2 1726.2\n",
      "- A.12  1   1642.6 1726.6\n",
      "- A.19  1   1643.8 1727.8\n",
      "- A.39  1   1644.1 1728.1\n",
      "- A.35  1   1644.3 1728.3\n",
      "- A.28  1   1644.8 1728.8\n",
      "- A.2   1   1645.0 1729.0\n",
      "- A.1   1   1645.2 1729.2\n",
      "- A.36  1   1645.3 1729.3\n",
      "- A.26  1   1646.2 1730.2\n",
      "- A.24  1   1647.8 1731.8\n",
      "- A.4   1   1647.9 1731.9\n",
      "- A.33  1   1648.9 1732.9\n",
      "- A.20  1   1650.0 1734.0\n",
      "- A.8   1   1650.5 1734.5\n",
      "- A.6   1   1650.5 1734.5\n",
      "- A.29  1   1651.0 1735.0\n",
      "- A.49  1   1654.4 1738.4\n",
      "- A.48  1   1654.5 1738.5\n",
      "- A.17  1   1656.2 1740.2\n",
      "- A.54  1   1657.5 1741.5\n",
      "- A.44  1   1658.5 1742.5\n",
      "- A.41  1   1660.0 1744.0\n",
      "- A.57  1   1664.0 1748.0\n",
      "- A.21  1   1664.8 1748.8\n",
      "- A.52  1   1667.5 1751.5\n",
      "- A.23  1   1671.6 1755.6\n",
      "- A.56  1   1671.9 1755.9\n",
      "- A.5   1   1676.5 1760.5\n",
      "- A.42  1   1683.2 1767.2\n",
      "- A.45  1   1688.3 1772.3\n",
      "- A.46  1   1699.6 1783.6\n",
      "- A.7   1   1722.0 1806.0\n",
      "- A.16  1   1727.6 1811.6\n",
      "- A.53  1   1746.6 1830.6\n",
      "- A.25  1   1772.8 1856.8\n",
      "- A.27  1   1884.7 1968.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:  glm(formula = spam ~ A.1 + A.2 + A.4 + A.5 + A.6 + A.7 + A.8 + \n",
       "    A.9 + A.10 + A.12 + A.15 + A.16 + A.17 + A.19 + A.20 + A.21 + \n",
       "    A.23 + A.24 + A.25 + A.26 + A.27 + A.28 + A.29 + A.33 + A.35 + \n",
       "    A.36 + A.38 + A.39 + A.41 + A.42 + A.43 + A.44 + A.45 + A.46 + \n",
       "    A.47 + A.48 + A.49 + A.52 + A.53 + A.54 + A.56 + A.57, family = binomial, \n",
       "    data = spam.train)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)          A.1          A.2          A.4          A.5          A.6  \n",
       "  -1.591408    -0.589773    -0.139307     2.804753     0.588000     0.817371  \n",
       "        A.7          A.8          A.9         A.10         A.12         A.15  \n",
       "   2.112301     0.476626     0.456388     0.113649    -0.159631     1.054460  \n",
       "       A.16         A.17         A.19         A.20         A.21         A.23  \n",
       "   1.136034     0.840119     0.089574     1.152511     0.274152     2.062435  \n",
       "       A.24         A.25         A.26         A.27         A.28         A.29  \n",
       "   0.369408    -1.970884    -1.071402   -10.680024     0.424638    -4.967787  \n",
       "       A.33         A.35         A.36         A.38         A.39         A.41  \n",
       "  -0.759343    -1.852855     0.885484    -0.667233    -0.831651   -46.686525  \n",
       "       A.42         A.43         A.44         A.45         A.46         A.47  \n",
       "  -2.635520    -1.293774    -1.686818    -0.854075    -1.435099    -2.224781  \n",
       "       A.48         A.49         A.52         A.53         A.54         A.56  \n",
       "  -4.239091    -0.933899     0.347232     6.544878     3.191417     0.009816  \n",
       "       A.57  \n",
       "   0.001040  \n",
       "\n",
       "Degrees of Freedom: 4140 Total (i.e. Null);  4098 Residual\n",
       "Null Deviance:\t    5553 \n",
       "Residual Deviance: 1638 \tAIC: 1724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step(spam.logist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "178"
      ],
      "text/latex": [
       "178"
      ],
      "text/markdown": [
       "178"
      ],
      "text/plain": [
       "[1] 178"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Type</th><th scope=col>Alcohol</th><th scope=col>Malic</th><th scope=col>Ash</th><th scope=col>Alcalinity</th><th scope=col>Magnesium</th><th scope=col>Phenols</th><th scope=col>Flavonoids</th><th scope=col>Nonflavonoids</th><th scope=col>Proanthocyanins</th><th scope=col>Color</th><th scope=col>Hue</th><th scope=col>Dilution</th><th scope=col>Proline</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1    </td><td>14.23</td><td>1.71 </td><td>2.43 </td><td>15.6 </td><td>127  </td><td>2.80 </td><td>3.06 </td><td>0.28 </td><td>2.29 </td><td>5.64 </td><td>1.04 </td><td>3.92 </td><td>1065 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.20</td><td>1.78 </td><td>2.14 </td><td>11.2 </td><td>100  </td><td>2.65 </td><td>2.76 </td><td>0.26 </td><td>1.28 </td><td>4.38 </td><td>1.05 </td><td>3.40 </td><td>1050 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.16</td><td>2.36 </td><td>2.67 </td><td>18.6 </td><td>101  </td><td>2.80 </td><td>3.24 </td><td>0.30 </td><td>2.81 </td><td>5.68 </td><td>1.03 </td><td>3.17 </td><td>1185 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.37</td><td>1.95 </td><td>2.50 </td><td>16.8 </td><td>113  </td><td>3.85 </td><td>3.49 </td><td>0.24 </td><td>2.18 </td><td>7.80 </td><td>0.86 </td><td>3.45 </td><td>1480 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.24</td><td>2.59 </td><td>2.87 </td><td>21.0 </td><td>118  </td><td>2.80 </td><td>2.69 </td><td>0.39 </td><td>1.82 </td><td>4.32 </td><td>1.04 </td><td>2.93 </td><td> 735 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.20</td><td>1.76 </td><td>2.45 </td><td>15.2 </td><td>112  </td><td>3.27 </td><td>3.39 </td><td>0.34 </td><td>1.97 </td><td>6.75 </td><td>1.05 </td><td>2.85 </td><td>1450 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.39</td><td>1.87 </td><td>2.45 </td><td>14.6 </td><td> 96  </td><td>2.50 </td><td>2.52 </td><td>0.30 </td><td>1.98 </td><td>5.25 </td><td>1.02 </td><td>3.58 </td><td>1290 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.06</td><td>2.15 </td><td>2.61 </td><td>17.6 </td><td>121  </td><td>2.60 </td><td>2.51 </td><td>0.31 </td><td>1.25 </td><td>5.05 </td><td>1.06 </td><td>3.58 </td><td>1295 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.83</td><td>1.64 </td><td>2.17 </td><td>14.0 </td><td> 97  </td><td>2.80 </td><td>2.98 </td><td>0.29 </td><td>1.98 </td><td>5.20 </td><td>1.08 </td><td>2.85 </td><td>1045 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.86</td><td>1.35 </td><td>2.27 </td><td>16.0 </td><td> 98  </td><td>2.98 </td><td>3.15 </td><td>0.22 </td><td>1.85 </td><td>7.22 </td><td>1.01 </td><td>3.55 </td><td>1045 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.10</td><td>2.16 </td><td>2.30 </td><td>18.0 </td><td>105  </td><td>2.95 </td><td>3.32 </td><td>0.22 </td><td>2.38 </td><td>5.75 </td><td>1.25 </td><td>3.17 </td><td>1510 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.12</td><td>1.48 </td><td>2.32 </td><td>16.8 </td><td> 95  </td><td>2.20 </td><td>2.43 </td><td>0.26 </td><td>1.57 </td><td>5.00 </td><td>1.17 </td><td>2.82 </td><td>1280 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.75</td><td>1.73 </td><td>2.41 </td><td>16.0 </td><td> 89  </td><td>2.60 </td><td>2.76 </td><td>0.29 </td><td>1.81 </td><td>5.60 </td><td>1.15 </td><td>2.90 </td><td>1320 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.75</td><td>1.73 </td><td>2.39 </td><td>11.4 </td><td> 91  </td><td>3.10 </td><td>3.69 </td><td>0.43 </td><td>2.81 </td><td>5.40 </td><td>1.25 </td><td>2.73 </td><td>1150 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.38</td><td>1.87 </td><td>2.38 </td><td>12.0 </td><td>102  </td><td>3.30 </td><td>3.64 </td><td>0.29 </td><td>2.96 </td><td>7.50 </td><td>1.20 </td><td>3.00 </td><td>1547 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.63</td><td>1.81 </td><td>2.70 </td><td>17.2 </td><td>112  </td><td>2.85 </td><td>2.91 </td><td>0.30 </td><td>1.46 </td><td>7.30 </td><td>1.28 </td><td>2.88 </td><td>1310 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.30</td><td>1.92 </td><td>2.72 </td><td>20.0 </td><td>120  </td><td>2.80 </td><td>3.14 </td><td>0.33 </td><td>1.97 </td><td>6.20 </td><td>1.07 </td><td>2.65 </td><td>1280 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.83</td><td>1.57 </td><td>2.62 </td><td>20.0 </td><td>115  </td><td>2.95 </td><td>3.40 </td><td>0.40 </td><td>1.72 </td><td>6.60 </td><td>1.13 </td><td>2.57 </td><td>1130 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.19</td><td>1.59 </td><td>2.48 </td><td>16.5 </td><td>108  </td><td>3.30 </td><td>3.93 </td><td>0.32 </td><td>1.86 </td><td>8.70 </td><td>1.23 </td><td>2.82 </td><td>1680 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.64</td><td>3.10 </td><td>2.56 </td><td>15.2 </td><td>116  </td><td>2.70 </td><td>3.03 </td><td>0.17 </td><td>1.66 </td><td>5.10 </td><td>0.96 </td><td>3.36 </td><td> 845 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.06</td><td>1.63 </td><td>2.28 </td><td>16.0 </td><td>126  </td><td>3.00 </td><td>3.17 </td><td>0.24 </td><td>2.10 </td><td>5.65 </td><td>1.09 </td><td>3.71 </td><td> 780 </td></tr>\n",
       "\t<tr><td>1    </td><td>12.93</td><td>3.80 </td><td>2.65 </td><td>18.6 </td><td>102  </td><td>2.41 </td><td>2.41 </td><td>0.25 </td><td>1.98 </td><td>4.50 </td><td>1.03 </td><td>3.52 </td><td> 770 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.71</td><td>1.86 </td><td>2.36 </td><td>16.6 </td><td>101  </td><td>2.61 </td><td>2.88 </td><td>0.27 </td><td>1.69 </td><td>3.80 </td><td>1.11 </td><td>4.00 </td><td>1035 </td></tr>\n",
       "\t<tr><td>1    </td><td>12.85</td><td>1.60 </td><td>2.52 </td><td>17.8 </td><td> 95  </td><td>2.48 </td><td>2.37 </td><td>0.26 </td><td>1.46 </td><td>3.93 </td><td>1.09 </td><td>3.63 </td><td>1015 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.50</td><td>1.81 </td><td>2.61 </td><td>20.0 </td><td> 96  </td><td>2.53 </td><td>2.61 </td><td>0.28 </td><td>1.66 </td><td>3.52 </td><td>1.12 </td><td>3.82 </td><td> 845 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.05</td><td>2.05 </td><td>3.22 </td><td>25.0 </td><td>124  </td><td>2.63 </td><td>2.68 </td><td>0.47 </td><td>1.92 </td><td>3.58 </td><td>1.13 </td><td>3.20 </td><td> 830 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.39</td><td>1.77 </td><td>2.62 </td><td>16.1 </td><td> 93  </td><td>2.85 </td><td>2.94 </td><td>0.34 </td><td>1.45 </td><td>4.80 </td><td>0.92 </td><td>3.22 </td><td>1195 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.30</td><td>1.72 </td><td>2.14 </td><td>17.0 </td><td> 94  </td><td>2.40 </td><td>2.19 </td><td>0.27 </td><td>1.35 </td><td>3.95 </td><td>1.02 </td><td>2.77 </td><td>1285 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.87</td><td>1.90 </td><td>2.80 </td><td>19.4 </td><td>107  </td><td>2.95 </td><td>2.97 </td><td>0.37 </td><td>1.76 </td><td>4.50 </td><td>1.25 </td><td>3.40 </td><td> 915 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.02</td><td>1.68 </td><td>2.21 </td><td>16.0 </td><td> 96  </td><td>2.65 </td><td>2.33 </td><td>0.26 </td><td>1.98 </td><td>4.70 </td><td>1.04 </td><td>3.59 </td><td>1035 </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>3        </td><td>13.32    </td><td>3.24     </td><td>2.38     </td><td>21.5     </td><td> 92      </td><td>1.93     </td><td>0.76     </td><td>0.45     </td><td>1.25     </td><td> 8.420000</td><td>0.55     </td><td>1.62     </td><td>650      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.08    </td><td>3.90     </td><td>2.36     </td><td>21.5     </td><td>113      </td><td>1.41     </td><td>1.39     </td><td>0.34     </td><td>1.14     </td><td> 9.400000</td><td>0.57     </td><td>1.33     </td><td>550      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.50    </td><td>3.12     </td><td>2.62     </td><td>24.0     </td><td>123      </td><td>1.40     </td><td>1.57     </td><td>0.22     </td><td>1.25     </td><td> 8.600000</td><td>0.59     </td><td>1.30     </td><td>500      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.79    </td><td>2.67     </td><td>2.48     </td><td>22.0     </td><td>112      </td><td>1.48     </td><td>1.36     </td><td>0.24     </td><td>1.26     </td><td>10.800000</td><td>0.48     </td><td>1.47     </td><td>480      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.11    </td><td>1.90     </td><td>2.75     </td><td>25.5     </td><td>116      </td><td>2.20     </td><td>1.28     </td><td>0.26     </td><td>1.56     </td><td> 7.100000</td><td>0.61     </td><td>1.33     </td><td>425      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.23    </td><td>3.30     </td><td>2.28     </td><td>18.5     </td><td> 98      </td><td>1.80     </td><td>0.83     </td><td>0.61     </td><td>1.87     </td><td>10.520000</td><td>0.56     </td><td>1.51     </td><td>675      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.58    </td><td>1.29     </td><td>2.10     </td><td>20.0     </td><td>103      </td><td>1.48     </td><td>0.58     </td><td>0.53     </td><td>1.40     </td><td> 7.600000</td><td>0.58     </td><td>1.55     </td><td>640      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.17    </td><td>5.19     </td><td>2.32     </td><td>22.0     </td><td> 93      </td><td>1.74     </td><td>0.63     </td><td>0.61     </td><td>1.55     </td><td> 7.900000</td><td>0.60     </td><td>1.48     </td><td>725      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.84    </td><td>4.12     </td><td>2.38     </td><td>19.5     </td><td> 89      </td><td>1.80     </td><td>0.83     </td><td>0.48     </td><td>1.56     </td><td> 9.010000</td><td>0.57     </td><td>1.64     </td><td>480      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.45    </td><td>3.03     </td><td>2.64     </td><td>27.0     </td><td> 97      </td><td>1.90     </td><td>0.58     </td><td>0.63     </td><td>1.14     </td><td> 7.500000</td><td>0.67     </td><td>1.73     </td><td>880      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.34    </td><td>1.68     </td><td>2.70     </td><td>25.0     </td><td> 98      </td><td>2.80     </td><td>1.31     </td><td>0.53     </td><td>2.70     </td><td>13.000000</td><td>0.57     </td><td>1.96     </td><td>660      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.48    </td><td>1.67     </td><td>2.64     </td><td>22.5     </td><td> 89      </td><td>2.60     </td><td>1.10     </td><td>0.52     </td><td>2.29     </td><td>11.750000</td><td>0.57     </td><td>1.78     </td><td>620      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.36    </td><td>3.83     </td><td>2.38     </td><td>21.0     </td><td> 88      </td><td>2.30     </td><td>0.92     </td><td>0.50     </td><td>1.04     </td><td> 7.650000</td><td>0.56     </td><td>1.58     </td><td>520      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.69    </td><td>3.26     </td><td>2.54     </td><td>20.0     </td><td>107      </td><td>1.83     </td><td>0.56     </td><td>0.50     </td><td>0.80     </td><td> 5.880000</td><td>0.96     </td><td>1.82     </td><td>680      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.85    </td><td>3.27     </td><td>2.58     </td><td>22.0     </td><td>106      </td><td>1.65     </td><td>0.60     </td><td>0.60     </td><td>0.96     </td><td> 5.580000</td><td>0.87     </td><td>2.11     </td><td>570      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.96    </td><td>3.45     </td><td>2.35     </td><td>18.5     </td><td>106      </td><td>1.39     </td><td>0.70     </td><td>0.40     </td><td>0.94     </td><td> 5.280000</td><td>0.68     </td><td>1.75     </td><td>675      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.78    </td><td>2.76     </td><td>2.30     </td><td>22.0     </td><td> 90      </td><td>1.35     </td><td>0.68     </td><td>0.41     </td><td>1.03     </td><td> 9.580000</td><td>0.70     </td><td>1.68     </td><td>615      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.73    </td><td>4.36     </td><td>2.26     </td><td>22.5     </td><td> 88      </td><td>1.28     </td><td>0.47     </td><td>0.52     </td><td>1.15     </td><td> 6.620000</td><td>0.78     </td><td>1.75     </td><td>520      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.45    </td><td>3.70     </td><td>2.60     </td><td>23.0     </td><td>111      </td><td>1.70     </td><td>0.92     </td><td>0.43     </td><td>1.46     </td><td>10.680000</td><td>0.85     </td><td>1.56     </td><td>695      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.82    </td><td>3.37     </td><td>2.30     </td><td>19.5     </td><td> 88      </td><td>1.48     </td><td>0.66     </td><td>0.40     </td><td>0.97     </td><td>10.260000</td><td>0.72     </td><td>1.75     </td><td>685      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.58    </td><td>2.58     </td><td>2.69     </td><td>24.5     </td><td>105      </td><td>1.55     </td><td>0.84     </td><td>0.39     </td><td>1.54     </td><td> 8.660000</td><td>0.74     </td><td>1.80     </td><td>750      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.40    </td><td>4.60     </td><td>2.86     </td><td>25.0     </td><td>112      </td><td>1.98     </td><td>0.96     </td><td>0.27     </td><td>1.11     </td><td> 8.500000</td><td>0.67     </td><td>1.92     </td><td>630      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.20    </td><td>3.03     </td><td>2.32     </td><td>19.0     </td><td> 96      </td><td>1.25     </td><td>0.49     </td><td>0.40     </td><td>0.73     </td><td> 5.500000</td><td>0.66     </td><td>1.83     </td><td>510      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.77    </td><td>2.39     </td><td>2.28     </td><td>19.5     </td><td> 86      </td><td>1.39     </td><td>0.51     </td><td>0.48     </td><td>0.64     </td><td> 9.899999</td><td>0.57     </td><td>1.63     </td><td>470      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.16    </td><td>2.51     </td><td>2.48     </td><td>20.0     </td><td> 91      </td><td>1.68     </td><td>0.70     </td><td>0.44     </td><td>1.24     </td><td> 9.700000</td><td>0.62     </td><td>1.71     </td><td>660      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.71    </td><td>5.65     </td><td>2.45     </td><td>20.5     </td><td> 95      </td><td>1.68     </td><td>0.61     </td><td>0.52     </td><td>1.06     </td><td> 7.700000</td><td>0.64     </td><td>1.74     </td><td>740      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.40    </td><td>3.91     </td><td>2.48     </td><td>23.0     </td><td>102      </td><td>1.80     </td><td>0.75     </td><td>0.43     </td><td>1.41     </td><td> 7.300000</td><td>0.70     </td><td>1.56     </td><td>750      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.27    </td><td>4.28     </td><td>2.26     </td><td>20.0     </td><td>120      </td><td>1.59     </td><td>0.69     </td><td>0.43     </td><td>1.35     </td><td>10.200000</td><td>0.59     </td><td>1.56     </td><td>835      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.17    </td><td>2.59     </td><td>2.37     </td><td>20.0     </td><td>120      </td><td>1.65     </td><td>0.68     </td><td>0.53     </td><td>1.46     </td><td> 9.300000</td><td>0.60     </td><td>1.62     </td><td>840      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.13    </td><td>4.10     </td><td>2.74     </td><td>24.5     </td><td> 96      </td><td>2.05     </td><td>0.76     </td><td>0.56     </td><td>1.35     </td><td> 9.200000</td><td>0.61     </td><td>1.60     </td><td>560      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " Type & Alcohol & Malic & Ash & Alcalinity & Magnesium & Phenols & Flavonoids & Nonflavonoids & Proanthocyanins & Color & Hue & Dilution & Proline\\\\\n",
       "\\hline\n",
       "\t 1     & 14.23 & 1.71  & 2.43  & 15.6  & 127   & 2.80  & 3.06  & 0.28  & 2.29  & 5.64  & 1.04  & 3.92  & 1065 \\\\\n",
       "\t 1     & 13.20 & 1.78  & 2.14  & 11.2  & 100   & 2.65  & 2.76  & 0.26  & 1.28  & 4.38  & 1.05  & 3.40  & 1050 \\\\\n",
       "\t 1     & 13.16 & 2.36  & 2.67  & 18.6  & 101   & 2.80  & 3.24  & 0.30  & 2.81  & 5.68  & 1.03  & 3.17  & 1185 \\\\\n",
       "\t 1     & 14.37 & 1.95  & 2.50  & 16.8  & 113   & 3.85  & 3.49  & 0.24  & 2.18  & 7.80  & 0.86  & 3.45  & 1480 \\\\\n",
       "\t 1     & 13.24 & 2.59  & 2.87  & 21.0  & 118   & 2.80  & 2.69  & 0.39  & 1.82  & 4.32  & 1.04  & 2.93  &  735 \\\\\n",
       "\t 1     & 14.20 & 1.76  & 2.45  & 15.2  & 112   & 3.27  & 3.39  & 0.34  & 1.97  & 6.75  & 1.05  & 2.85  & 1450 \\\\\n",
       "\t 1     & 14.39 & 1.87  & 2.45  & 14.6  &  96   & 2.50  & 2.52  & 0.30  & 1.98  & 5.25  & 1.02  & 3.58  & 1290 \\\\\n",
       "\t 1     & 14.06 & 2.15  & 2.61  & 17.6  & 121   & 2.60  & 2.51  & 0.31  & 1.25  & 5.05  & 1.06  & 3.58  & 1295 \\\\\n",
       "\t 1     & 14.83 & 1.64  & 2.17  & 14.0  &  97   & 2.80  & 2.98  & 0.29  & 1.98  & 5.20  & 1.08  & 2.85  & 1045 \\\\\n",
       "\t 1     & 13.86 & 1.35  & 2.27  & 16.0  &  98   & 2.98  & 3.15  & 0.22  & 1.85  & 7.22  & 1.01  & 3.55  & 1045 \\\\\n",
       "\t 1     & 14.10 & 2.16  & 2.30  & 18.0  & 105   & 2.95  & 3.32  & 0.22  & 2.38  & 5.75  & 1.25  & 3.17  & 1510 \\\\\n",
       "\t 1     & 14.12 & 1.48  & 2.32  & 16.8  &  95   & 2.20  & 2.43  & 0.26  & 1.57  & 5.00  & 1.17  & 2.82  & 1280 \\\\\n",
       "\t 1     & 13.75 & 1.73  & 2.41  & 16.0  &  89   & 2.60  & 2.76  & 0.29  & 1.81  & 5.60  & 1.15  & 2.90  & 1320 \\\\\n",
       "\t 1     & 14.75 & 1.73  & 2.39  & 11.4  &  91   & 3.10  & 3.69  & 0.43  & 2.81  & 5.40  & 1.25  & 2.73  & 1150 \\\\\n",
       "\t 1     & 14.38 & 1.87  & 2.38  & 12.0  & 102   & 3.30  & 3.64  & 0.29  & 2.96  & 7.50  & 1.20  & 3.00  & 1547 \\\\\n",
       "\t 1     & 13.63 & 1.81  & 2.70  & 17.2  & 112   & 2.85  & 2.91  & 0.30  & 1.46  & 7.30  & 1.28  & 2.88  & 1310 \\\\\n",
       "\t 1     & 14.30 & 1.92  & 2.72  & 20.0  & 120   & 2.80  & 3.14  & 0.33  & 1.97  & 6.20  & 1.07  & 2.65  & 1280 \\\\\n",
       "\t 1     & 13.83 & 1.57  & 2.62  & 20.0  & 115   & 2.95  & 3.40  & 0.40  & 1.72  & 6.60  & 1.13  & 2.57  & 1130 \\\\\n",
       "\t 1     & 14.19 & 1.59  & 2.48  & 16.5  & 108   & 3.30  & 3.93  & 0.32  & 1.86  & 8.70  & 1.23  & 2.82  & 1680 \\\\\n",
       "\t 1     & 13.64 & 3.10  & 2.56  & 15.2  & 116   & 2.70  & 3.03  & 0.17  & 1.66  & 5.10  & 0.96  & 3.36  &  845 \\\\\n",
       "\t 1     & 14.06 & 1.63  & 2.28  & 16.0  & 126   & 3.00  & 3.17  & 0.24  & 2.10  & 5.65  & 1.09  & 3.71  &  780 \\\\\n",
       "\t 1     & 12.93 & 3.80  & 2.65  & 18.6  & 102   & 2.41  & 2.41  & 0.25  & 1.98  & 4.50  & 1.03  & 3.52  &  770 \\\\\n",
       "\t 1     & 13.71 & 1.86  & 2.36  & 16.6  & 101   & 2.61  & 2.88  & 0.27  & 1.69  & 3.80  & 1.11  & 4.00  & 1035 \\\\\n",
       "\t 1     & 12.85 & 1.60  & 2.52  & 17.8  &  95   & 2.48  & 2.37  & 0.26  & 1.46  & 3.93  & 1.09  & 3.63  & 1015 \\\\\n",
       "\t 1     & 13.50 & 1.81  & 2.61  & 20.0  &  96   & 2.53  & 2.61  & 0.28  & 1.66  & 3.52  & 1.12  & 3.82  &  845 \\\\\n",
       "\t 1     & 13.05 & 2.05  & 3.22  & 25.0  & 124   & 2.63  & 2.68  & 0.47  & 1.92  & 3.58  & 1.13  & 3.20  &  830 \\\\\n",
       "\t 1     & 13.39 & 1.77  & 2.62  & 16.1  &  93   & 2.85  & 2.94  & 0.34  & 1.45  & 4.80  & 0.92  & 3.22  & 1195 \\\\\n",
       "\t 1     & 13.30 & 1.72  & 2.14  & 17.0  &  94   & 2.40  & 2.19  & 0.27  & 1.35  & 3.95  & 1.02  & 2.77  & 1285 \\\\\n",
       "\t 1     & 13.87 & 1.90  & 2.80  & 19.4  & 107   & 2.95  & 2.97  & 0.37  & 1.76  & 4.50  & 1.25  & 3.40  &  915 \\\\\n",
       "\t 1     & 14.02 & 1.68  & 2.21  & 16.0  &  96   & 2.65  & 2.33  & 0.26  & 1.98  & 4.70  & 1.04  & 3.59  & 1035 \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t 3         & 13.32     & 3.24      & 2.38      & 21.5      &  92       & 1.93      & 0.76      & 0.45      & 1.25      &  8.420000 & 0.55      & 1.62      & 650      \\\\\n",
       "\t 3         & 13.08     & 3.90      & 2.36      & 21.5      & 113       & 1.41      & 1.39      & 0.34      & 1.14      &  9.400000 & 0.57      & 1.33      & 550      \\\\\n",
       "\t 3         & 13.50     & 3.12      & 2.62      & 24.0      & 123       & 1.40      & 1.57      & 0.22      & 1.25      &  8.600000 & 0.59      & 1.30      & 500      \\\\\n",
       "\t 3         & 12.79     & 2.67      & 2.48      & 22.0      & 112       & 1.48      & 1.36      & 0.24      & 1.26      & 10.800000 & 0.48      & 1.47      & 480      \\\\\n",
       "\t 3         & 13.11     & 1.90      & 2.75      & 25.5      & 116       & 2.20      & 1.28      & 0.26      & 1.56      &  7.100000 & 0.61      & 1.33      & 425      \\\\\n",
       "\t 3         & 13.23     & 3.30      & 2.28      & 18.5      &  98       & 1.80      & 0.83      & 0.61      & 1.87      & 10.520000 & 0.56      & 1.51      & 675      \\\\\n",
       "\t 3         & 12.58     & 1.29      & 2.10      & 20.0      & 103       & 1.48      & 0.58      & 0.53      & 1.40      &  7.600000 & 0.58      & 1.55      & 640      \\\\\n",
       "\t 3         & 13.17     & 5.19      & 2.32      & 22.0      &  93       & 1.74      & 0.63      & 0.61      & 1.55      &  7.900000 & 0.60      & 1.48      & 725      \\\\\n",
       "\t 3         & 13.84     & 4.12      & 2.38      & 19.5      &  89       & 1.80      & 0.83      & 0.48      & 1.56      &  9.010000 & 0.57      & 1.64      & 480      \\\\\n",
       "\t 3         & 12.45     & 3.03      & 2.64      & 27.0      &  97       & 1.90      & 0.58      & 0.63      & 1.14      &  7.500000 & 0.67      & 1.73      & 880      \\\\\n",
       "\t 3         & 14.34     & 1.68      & 2.70      & 25.0      &  98       & 2.80      & 1.31      & 0.53      & 2.70      & 13.000000 & 0.57      & 1.96      & 660      \\\\\n",
       "\t 3         & 13.48     & 1.67      & 2.64      & 22.5      &  89       & 2.60      & 1.10      & 0.52      & 2.29      & 11.750000 & 0.57      & 1.78      & 620      \\\\\n",
       "\t 3         & 12.36     & 3.83      & 2.38      & 21.0      &  88       & 2.30      & 0.92      & 0.50      & 1.04      &  7.650000 & 0.56      & 1.58      & 520      \\\\\n",
       "\t 3         & 13.69     & 3.26      & 2.54      & 20.0      & 107       & 1.83      & 0.56      & 0.50      & 0.80      &  5.880000 & 0.96      & 1.82      & 680      \\\\\n",
       "\t 3         & 12.85     & 3.27      & 2.58      & 22.0      & 106       & 1.65      & 0.60      & 0.60      & 0.96      &  5.580000 & 0.87      & 2.11      & 570      \\\\\n",
       "\t 3         & 12.96     & 3.45      & 2.35      & 18.5      & 106       & 1.39      & 0.70      & 0.40      & 0.94      &  5.280000 & 0.68      & 1.75      & 675      \\\\\n",
       "\t 3         & 13.78     & 2.76      & 2.30      & 22.0      &  90       & 1.35      & 0.68      & 0.41      & 1.03      &  9.580000 & 0.70      & 1.68      & 615      \\\\\n",
       "\t 3         & 13.73     & 4.36      & 2.26      & 22.5      &  88       & 1.28      & 0.47      & 0.52      & 1.15      &  6.620000 & 0.78      & 1.75      & 520      \\\\\n",
       "\t 3         & 13.45     & 3.70      & 2.60      & 23.0      & 111       & 1.70      & 0.92      & 0.43      & 1.46      & 10.680000 & 0.85      & 1.56      & 695      \\\\\n",
       "\t 3         & 12.82     & 3.37      & 2.30      & 19.5      &  88       & 1.48      & 0.66      & 0.40      & 0.97      & 10.260000 & 0.72      & 1.75      & 685      \\\\\n",
       "\t 3         & 13.58     & 2.58      & 2.69      & 24.5      & 105       & 1.55      & 0.84      & 0.39      & 1.54      &  8.660000 & 0.74      & 1.80      & 750      \\\\\n",
       "\t 3         & 13.40     & 4.60      & 2.86      & 25.0      & 112       & 1.98      & 0.96      & 0.27      & 1.11      &  8.500000 & 0.67      & 1.92      & 630      \\\\\n",
       "\t 3         & 12.20     & 3.03      & 2.32      & 19.0      &  96       & 1.25      & 0.49      & 0.40      & 0.73      &  5.500000 & 0.66      & 1.83      & 510      \\\\\n",
       "\t 3         & 12.77     & 2.39      & 2.28      & 19.5      &  86       & 1.39      & 0.51      & 0.48      & 0.64      &  9.899999 & 0.57      & 1.63      & 470      \\\\\n",
       "\t 3         & 14.16     & 2.51      & 2.48      & 20.0      &  91       & 1.68      & 0.70      & 0.44      & 1.24      &  9.700000 & 0.62      & 1.71      & 660      \\\\\n",
       "\t 3         & 13.71     & 5.65      & 2.45      & 20.5      &  95       & 1.68      & 0.61      & 0.52      & 1.06      &  7.700000 & 0.64      & 1.74      & 740      \\\\\n",
       "\t 3         & 13.40     & 3.91      & 2.48      & 23.0      & 102       & 1.80      & 0.75      & 0.43      & 1.41      &  7.300000 & 0.70      & 1.56      & 750      \\\\\n",
       "\t 3         & 13.27     & 4.28      & 2.26      & 20.0      & 120       & 1.59      & 0.69      & 0.43      & 1.35      & 10.200000 & 0.59      & 1.56      & 835      \\\\\n",
       "\t 3         & 13.17     & 2.59      & 2.37      & 20.0      & 120       & 1.65      & 0.68      & 0.53      & 1.46      &  9.300000 & 0.60      & 1.62      & 840      \\\\\n",
       "\t 3         & 14.13     & 4.10      & 2.74      & 24.5      &  96       & 2.05      & 0.76      & 0.56      & 1.35      &  9.200000 & 0.61      & 1.60      & 560      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Type | Alcohol | Malic | Ash | Alcalinity | Magnesium | Phenols | Flavonoids | Nonflavonoids | Proanthocyanins | Color | Hue | Dilution | Proline | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1     | 14.23 | 1.71  | 2.43  | 15.6  | 127   | 2.80  | 3.06  | 0.28  | 2.29  | 5.64  | 1.04  | 3.92  | 1065  | \n",
       "| 1     | 13.20 | 1.78  | 2.14  | 11.2  | 100   | 2.65  | 2.76  | 0.26  | 1.28  | 4.38  | 1.05  | 3.40  | 1050  | \n",
       "| 1     | 13.16 | 2.36  | 2.67  | 18.6  | 101   | 2.80  | 3.24  | 0.30  | 2.81  | 5.68  | 1.03  | 3.17  | 1185  | \n",
       "| 1     | 14.37 | 1.95  | 2.50  | 16.8  | 113   | 3.85  | 3.49  | 0.24  | 2.18  | 7.80  | 0.86  | 3.45  | 1480  | \n",
       "| 1     | 13.24 | 2.59  | 2.87  | 21.0  | 118   | 2.80  | 2.69  | 0.39  | 1.82  | 4.32  | 1.04  | 2.93  |  735  | \n",
       "| 1     | 14.20 | 1.76  | 2.45  | 15.2  | 112   | 3.27  | 3.39  | 0.34  | 1.97  | 6.75  | 1.05  | 2.85  | 1450  | \n",
       "| 1     | 14.39 | 1.87  | 2.45  | 14.6  |  96   | 2.50  | 2.52  | 0.30  | 1.98  | 5.25  | 1.02  | 3.58  | 1290  | \n",
       "| 1     | 14.06 | 2.15  | 2.61  | 17.6  | 121   | 2.60  | 2.51  | 0.31  | 1.25  | 5.05  | 1.06  | 3.58  | 1295  | \n",
       "| 1     | 14.83 | 1.64  | 2.17  | 14.0  |  97   | 2.80  | 2.98  | 0.29  | 1.98  | 5.20  | 1.08  | 2.85  | 1045  | \n",
       "| 1     | 13.86 | 1.35  | 2.27  | 16.0  |  98   | 2.98  | 3.15  | 0.22  | 1.85  | 7.22  | 1.01  | 3.55  | 1045  | \n",
       "| 1     | 14.10 | 2.16  | 2.30  | 18.0  | 105   | 2.95  | 3.32  | 0.22  | 2.38  | 5.75  | 1.25  | 3.17  | 1510  | \n",
       "| 1     | 14.12 | 1.48  | 2.32  | 16.8  |  95   | 2.20  | 2.43  | 0.26  | 1.57  | 5.00  | 1.17  | 2.82  | 1280  | \n",
       "| 1     | 13.75 | 1.73  | 2.41  | 16.0  |  89   | 2.60  | 2.76  | 0.29  | 1.81  | 5.60  | 1.15  | 2.90  | 1320  | \n",
       "| 1     | 14.75 | 1.73  | 2.39  | 11.4  |  91   | 3.10  | 3.69  | 0.43  | 2.81  | 5.40  | 1.25  | 2.73  | 1150  | \n",
       "| 1     | 14.38 | 1.87  | 2.38  | 12.0  | 102   | 3.30  | 3.64  | 0.29  | 2.96  | 7.50  | 1.20  | 3.00  | 1547  | \n",
       "| 1     | 13.63 | 1.81  | 2.70  | 17.2  | 112   | 2.85  | 2.91  | 0.30  | 1.46  | 7.30  | 1.28  | 2.88  | 1310  | \n",
       "| 1     | 14.30 | 1.92  | 2.72  | 20.0  | 120   | 2.80  | 3.14  | 0.33  | 1.97  | 6.20  | 1.07  | 2.65  | 1280  | \n",
       "| 1     | 13.83 | 1.57  | 2.62  | 20.0  | 115   | 2.95  | 3.40  | 0.40  | 1.72  | 6.60  | 1.13  | 2.57  | 1130  | \n",
       "| 1     | 14.19 | 1.59  | 2.48  | 16.5  | 108   | 3.30  | 3.93  | 0.32  | 1.86  | 8.70  | 1.23  | 2.82  | 1680  | \n",
       "| 1     | 13.64 | 3.10  | 2.56  | 15.2  | 116   | 2.70  | 3.03  | 0.17  | 1.66  | 5.10  | 0.96  | 3.36  |  845  | \n",
       "| 1     | 14.06 | 1.63  | 2.28  | 16.0  | 126   | 3.00  | 3.17  | 0.24  | 2.10  | 5.65  | 1.09  | 3.71  |  780  | \n",
       "| 1     | 12.93 | 3.80  | 2.65  | 18.6  | 102   | 2.41  | 2.41  | 0.25  | 1.98  | 4.50  | 1.03  | 3.52  |  770  | \n",
       "| 1     | 13.71 | 1.86  | 2.36  | 16.6  | 101   | 2.61  | 2.88  | 0.27  | 1.69  | 3.80  | 1.11  | 4.00  | 1035  | \n",
       "| 1     | 12.85 | 1.60  | 2.52  | 17.8  |  95   | 2.48  | 2.37  | 0.26  | 1.46  | 3.93  | 1.09  | 3.63  | 1015  | \n",
       "| 1     | 13.50 | 1.81  | 2.61  | 20.0  |  96   | 2.53  | 2.61  | 0.28  | 1.66  | 3.52  | 1.12  | 3.82  |  845  | \n",
       "| 1     | 13.05 | 2.05  | 3.22  | 25.0  | 124   | 2.63  | 2.68  | 0.47  | 1.92  | 3.58  | 1.13  | 3.20  |  830  | \n",
       "| 1     | 13.39 | 1.77  | 2.62  | 16.1  |  93   | 2.85  | 2.94  | 0.34  | 1.45  | 4.80  | 0.92  | 3.22  | 1195  | \n",
       "| 1     | 13.30 | 1.72  | 2.14  | 17.0  |  94   | 2.40  | 2.19  | 0.27  | 1.35  | 3.95  | 1.02  | 2.77  | 1285  | \n",
       "| 1     | 13.87 | 1.90  | 2.80  | 19.4  | 107   | 2.95  | 2.97  | 0.37  | 1.76  | 4.50  | 1.25  | 3.40  |  915  | \n",
       "| 1     | 14.02 | 1.68  | 2.21  | 16.0  |  96   | 2.65  | 2.33  | 0.26  | 1.98  | 4.70  | 1.04  | 3.59  | 1035  | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| 3         | 13.32     | 3.24      | 2.38      | 21.5      |  92       | 1.93      | 0.76      | 0.45      | 1.25      |  8.420000 | 0.55      | 1.62      | 650       | \n",
       "| 3         | 13.08     | 3.90      | 2.36      | 21.5      | 113       | 1.41      | 1.39      | 0.34      | 1.14      |  9.400000 | 0.57      | 1.33      | 550       | \n",
       "| 3         | 13.50     | 3.12      | 2.62      | 24.0      | 123       | 1.40      | 1.57      | 0.22      | 1.25      |  8.600000 | 0.59      | 1.30      | 500       | \n",
       "| 3         | 12.79     | 2.67      | 2.48      | 22.0      | 112       | 1.48      | 1.36      | 0.24      | 1.26      | 10.800000 | 0.48      | 1.47      | 480       | \n",
       "| 3         | 13.11     | 1.90      | 2.75      | 25.5      | 116       | 2.20      | 1.28      | 0.26      | 1.56      |  7.100000 | 0.61      | 1.33      | 425       | \n",
       "| 3         | 13.23     | 3.30      | 2.28      | 18.5      |  98       | 1.80      | 0.83      | 0.61      | 1.87      | 10.520000 | 0.56      | 1.51      | 675       | \n",
       "| 3         | 12.58     | 1.29      | 2.10      | 20.0      | 103       | 1.48      | 0.58      | 0.53      | 1.40      |  7.600000 | 0.58      | 1.55      | 640       | \n",
       "| 3         | 13.17     | 5.19      | 2.32      | 22.0      |  93       | 1.74      | 0.63      | 0.61      | 1.55      |  7.900000 | 0.60      | 1.48      | 725       | \n",
       "| 3         | 13.84     | 4.12      | 2.38      | 19.5      |  89       | 1.80      | 0.83      | 0.48      | 1.56      |  9.010000 | 0.57      | 1.64      | 480       | \n",
       "| 3         | 12.45     | 3.03      | 2.64      | 27.0      |  97       | 1.90      | 0.58      | 0.63      | 1.14      |  7.500000 | 0.67      | 1.73      | 880       | \n",
       "| 3         | 14.34     | 1.68      | 2.70      | 25.0      |  98       | 2.80      | 1.31      | 0.53      | 2.70      | 13.000000 | 0.57      | 1.96      | 660       | \n",
       "| 3         | 13.48     | 1.67      | 2.64      | 22.5      |  89       | 2.60      | 1.10      | 0.52      | 2.29      | 11.750000 | 0.57      | 1.78      | 620       | \n",
       "| 3         | 12.36     | 3.83      | 2.38      | 21.0      |  88       | 2.30      | 0.92      | 0.50      | 1.04      |  7.650000 | 0.56      | 1.58      | 520       | \n",
       "| 3         | 13.69     | 3.26      | 2.54      | 20.0      | 107       | 1.83      | 0.56      | 0.50      | 0.80      |  5.880000 | 0.96      | 1.82      | 680       | \n",
       "| 3         | 12.85     | 3.27      | 2.58      | 22.0      | 106       | 1.65      | 0.60      | 0.60      | 0.96      |  5.580000 | 0.87      | 2.11      | 570       | \n",
       "| 3         | 12.96     | 3.45      | 2.35      | 18.5      | 106       | 1.39      | 0.70      | 0.40      | 0.94      |  5.280000 | 0.68      | 1.75      | 675       | \n",
       "| 3         | 13.78     | 2.76      | 2.30      | 22.0      |  90       | 1.35      | 0.68      | 0.41      | 1.03      |  9.580000 | 0.70      | 1.68      | 615       | \n",
       "| 3         | 13.73     | 4.36      | 2.26      | 22.5      |  88       | 1.28      | 0.47      | 0.52      | 1.15      |  6.620000 | 0.78      | 1.75      | 520       | \n",
       "| 3         | 13.45     | 3.70      | 2.60      | 23.0      | 111       | 1.70      | 0.92      | 0.43      | 1.46      | 10.680000 | 0.85      | 1.56      | 695       | \n",
       "| 3         | 12.82     | 3.37      | 2.30      | 19.5      |  88       | 1.48      | 0.66      | 0.40      | 0.97      | 10.260000 | 0.72      | 1.75      | 685       | \n",
       "| 3         | 13.58     | 2.58      | 2.69      | 24.5      | 105       | 1.55      | 0.84      | 0.39      | 1.54      |  8.660000 | 0.74      | 1.80      | 750       | \n",
       "| 3         | 13.40     | 4.60      | 2.86      | 25.0      | 112       | 1.98      | 0.96      | 0.27      | 1.11      |  8.500000 | 0.67      | 1.92      | 630       | \n",
       "| 3         | 12.20     | 3.03      | 2.32      | 19.0      |  96       | 1.25      | 0.49      | 0.40      | 0.73      |  5.500000 | 0.66      | 1.83      | 510       | \n",
       "| 3         | 12.77     | 2.39      | 2.28      | 19.5      |  86       | 1.39      | 0.51      | 0.48      | 0.64      |  9.899999 | 0.57      | 1.63      | 470       | \n",
       "| 3         | 14.16     | 2.51      | 2.48      | 20.0      |  91       | 1.68      | 0.70      | 0.44      | 1.24      |  9.700000 | 0.62      | 1.71      | 660       | \n",
       "| 3         | 13.71     | 5.65      | 2.45      | 20.5      |  95       | 1.68      | 0.61      | 0.52      | 1.06      |  7.700000 | 0.64      | 1.74      | 740       | \n",
       "| 3         | 13.40     | 3.91      | 2.48      | 23.0      | 102       | 1.80      | 0.75      | 0.43      | 1.41      |  7.300000 | 0.70      | 1.56      | 750       | \n",
       "| 3         | 13.27     | 4.28      | 2.26      | 20.0      | 120       | 1.59      | 0.69      | 0.43      | 1.35      | 10.200000 | 0.59      | 1.56      | 835       | \n",
       "| 3         | 13.17     | 2.59      | 2.37      | 20.0      | 120       | 1.65      | 0.68      | 0.53      | 1.46      |  9.300000 | 0.60      | 1.62      | 840       | \n",
       "| 3         | 14.13     | 4.10      | 2.74      | 24.5      |  96       | 2.05      | 0.76      | 0.56      | 1.35      |  9.200000 | 0.61      | 1.60      | 560       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Type Alcohol Malic Ash  Alcalinity Magnesium Phenols Flavonoids\n",
       "1   1    14.23   1.71  2.43 15.6       127       2.80    3.06      \n",
       "2   1    13.20   1.78  2.14 11.2       100       2.65    2.76      \n",
       "3   1    13.16   2.36  2.67 18.6       101       2.80    3.24      \n",
       "4   1    14.37   1.95  2.50 16.8       113       3.85    3.49      \n",
       "5   1    13.24   2.59  2.87 21.0       118       2.80    2.69      \n",
       "6   1    14.20   1.76  2.45 15.2       112       3.27    3.39      \n",
       "7   1    14.39   1.87  2.45 14.6        96       2.50    2.52      \n",
       "8   1    14.06   2.15  2.61 17.6       121       2.60    2.51      \n",
       "9   1    14.83   1.64  2.17 14.0        97       2.80    2.98      \n",
       "10  1    13.86   1.35  2.27 16.0        98       2.98    3.15      \n",
       "11  1    14.10   2.16  2.30 18.0       105       2.95    3.32      \n",
       "12  1    14.12   1.48  2.32 16.8        95       2.20    2.43      \n",
       "13  1    13.75   1.73  2.41 16.0        89       2.60    2.76      \n",
       "14  1    14.75   1.73  2.39 11.4        91       3.10    3.69      \n",
       "15  1    14.38   1.87  2.38 12.0       102       3.30    3.64      \n",
       "16  1    13.63   1.81  2.70 17.2       112       2.85    2.91      \n",
       "17  1    14.30   1.92  2.72 20.0       120       2.80    3.14      \n",
       "18  1    13.83   1.57  2.62 20.0       115       2.95    3.40      \n",
       "19  1    14.19   1.59  2.48 16.5       108       3.30    3.93      \n",
       "20  1    13.64   3.10  2.56 15.2       116       2.70    3.03      \n",
       "21  1    14.06   1.63  2.28 16.0       126       3.00    3.17      \n",
       "22  1    12.93   3.80  2.65 18.6       102       2.41    2.41      \n",
       "23  1    13.71   1.86  2.36 16.6       101       2.61    2.88      \n",
       "24  1    12.85   1.60  2.52 17.8        95       2.48    2.37      \n",
       "25  1    13.50   1.81  2.61 20.0        96       2.53    2.61      \n",
       "26  1    13.05   2.05  3.22 25.0       124       2.63    2.68      \n",
       "27  1    13.39   1.77  2.62 16.1        93       2.85    2.94      \n",
       "28  1    13.30   1.72  2.14 17.0        94       2.40    2.19      \n",
       "29  1    13.87   1.90  2.80 19.4       107       2.95    2.97      \n",
       "30  1    14.02   1.68  2.21 16.0        96       2.65    2.33      \n",
       "... ...  ...     ...   ...  ...        ...       ...     ...       \n",
       "149 3    13.32   3.24  2.38 21.5        92       1.93    0.76      \n",
       "150 3    13.08   3.90  2.36 21.5       113       1.41    1.39      \n",
       "151 3    13.50   3.12  2.62 24.0       123       1.40    1.57      \n",
       "152 3    12.79   2.67  2.48 22.0       112       1.48    1.36      \n",
       "153 3    13.11   1.90  2.75 25.5       116       2.20    1.28      \n",
       "154 3    13.23   3.30  2.28 18.5        98       1.80    0.83      \n",
       "155 3    12.58   1.29  2.10 20.0       103       1.48    0.58      \n",
       "156 3    13.17   5.19  2.32 22.0        93       1.74    0.63      \n",
       "157 3    13.84   4.12  2.38 19.5        89       1.80    0.83      \n",
       "158 3    12.45   3.03  2.64 27.0        97       1.90    0.58      \n",
       "159 3    14.34   1.68  2.70 25.0        98       2.80    1.31      \n",
       "160 3    13.48   1.67  2.64 22.5        89       2.60    1.10      \n",
       "161 3    12.36   3.83  2.38 21.0        88       2.30    0.92      \n",
       "162 3    13.69   3.26  2.54 20.0       107       1.83    0.56      \n",
       "163 3    12.85   3.27  2.58 22.0       106       1.65    0.60      \n",
       "164 3    12.96   3.45  2.35 18.5       106       1.39    0.70      \n",
       "165 3    13.78   2.76  2.30 22.0        90       1.35    0.68      \n",
       "166 3    13.73   4.36  2.26 22.5        88       1.28    0.47      \n",
       "167 3    13.45   3.70  2.60 23.0       111       1.70    0.92      \n",
       "168 3    12.82   3.37  2.30 19.5        88       1.48    0.66      \n",
       "169 3    13.58   2.58  2.69 24.5       105       1.55    0.84      \n",
       "170 3    13.40   4.60  2.86 25.0       112       1.98    0.96      \n",
       "171 3    12.20   3.03  2.32 19.0        96       1.25    0.49      \n",
       "172 3    12.77   2.39  2.28 19.5        86       1.39    0.51      \n",
       "173 3    14.16   2.51  2.48 20.0        91       1.68    0.70      \n",
       "174 3    13.71   5.65  2.45 20.5        95       1.68    0.61      \n",
       "175 3    13.40   3.91  2.48 23.0       102       1.80    0.75      \n",
       "176 3    13.27   4.28  2.26 20.0       120       1.59    0.69      \n",
       "177 3    13.17   2.59  2.37 20.0       120       1.65    0.68      \n",
       "178 3    14.13   4.10  2.74 24.5        96       2.05    0.76      \n",
       "    Nonflavonoids Proanthocyanins Color     Hue  Dilution Proline\n",
       "1   0.28          2.29            5.64      1.04 3.92     1065   \n",
       "2   0.26          1.28            4.38      1.05 3.40     1050   \n",
       "3   0.30          2.81            5.68      1.03 3.17     1185   \n",
       "4   0.24          2.18            7.80      0.86 3.45     1480   \n",
       "5   0.39          1.82            4.32      1.04 2.93      735   \n",
       "6   0.34          1.97            6.75      1.05 2.85     1450   \n",
       "7   0.30          1.98            5.25      1.02 3.58     1290   \n",
       "8   0.31          1.25            5.05      1.06 3.58     1295   \n",
       "9   0.29          1.98            5.20      1.08 2.85     1045   \n",
       "10  0.22          1.85            7.22      1.01 3.55     1045   \n",
       "11  0.22          2.38            5.75      1.25 3.17     1510   \n",
       "12  0.26          1.57            5.00      1.17 2.82     1280   \n",
       "13  0.29          1.81            5.60      1.15 2.90     1320   \n",
       "14  0.43          2.81            5.40      1.25 2.73     1150   \n",
       "15  0.29          2.96            7.50      1.20 3.00     1547   \n",
       "16  0.30          1.46            7.30      1.28 2.88     1310   \n",
       "17  0.33          1.97            6.20      1.07 2.65     1280   \n",
       "18  0.40          1.72            6.60      1.13 2.57     1130   \n",
       "19  0.32          1.86            8.70      1.23 2.82     1680   \n",
       "20  0.17          1.66            5.10      0.96 3.36      845   \n",
       "21  0.24          2.10            5.65      1.09 3.71      780   \n",
       "22  0.25          1.98            4.50      1.03 3.52      770   \n",
       "23  0.27          1.69            3.80      1.11 4.00     1035   \n",
       "24  0.26          1.46            3.93      1.09 3.63     1015   \n",
       "25  0.28          1.66            3.52      1.12 3.82      845   \n",
       "26  0.47          1.92            3.58      1.13 3.20      830   \n",
       "27  0.34          1.45            4.80      0.92 3.22     1195   \n",
       "28  0.27          1.35            3.95      1.02 2.77     1285   \n",
       "29  0.37          1.76            4.50      1.25 3.40      915   \n",
       "30  0.26          1.98            4.70      1.04 3.59     1035   \n",
       "... ...           ...             ...       ...  ...      ...    \n",
       "149 0.45          1.25             8.420000 0.55 1.62     650    \n",
       "150 0.34          1.14             9.400000 0.57 1.33     550    \n",
       "151 0.22          1.25             8.600000 0.59 1.30     500    \n",
       "152 0.24          1.26            10.800000 0.48 1.47     480    \n",
       "153 0.26          1.56             7.100000 0.61 1.33     425    \n",
       "154 0.61          1.87            10.520000 0.56 1.51     675    \n",
       "155 0.53          1.40             7.600000 0.58 1.55     640    \n",
       "156 0.61          1.55             7.900000 0.60 1.48     725    \n",
       "157 0.48          1.56             9.010000 0.57 1.64     480    \n",
       "158 0.63          1.14             7.500000 0.67 1.73     880    \n",
       "159 0.53          2.70            13.000000 0.57 1.96     660    \n",
       "160 0.52          2.29            11.750000 0.57 1.78     620    \n",
       "161 0.50          1.04             7.650000 0.56 1.58     520    \n",
       "162 0.50          0.80             5.880000 0.96 1.82     680    \n",
       "163 0.60          0.96             5.580000 0.87 2.11     570    \n",
       "164 0.40          0.94             5.280000 0.68 1.75     675    \n",
       "165 0.41          1.03             9.580000 0.70 1.68     615    \n",
       "166 0.52          1.15             6.620000 0.78 1.75     520    \n",
       "167 0.43          1.46            10.680000 0.85 1.56     695    \n",
       "168 0.40          0.97            10.260000 0.72 1.75     685    \n",
       "169 0.39          1.54             8.660000 0.74 1.80     750    \n",
       "170 0.27          1.11             8.500000 0.67 1.92     630    \n",
       "171 0.40          0.73             5.500000 0.66 1.83     510    \n",
       "172 0.48          0.64             9.899999 0.57 1.63     470    \n",
       "173 0.44          1.24             9.700000 0.62 1.71     660    \n",
       "174 0.52          1.06             7.700000 0.64 1.74     740    \n",
       "175 0.43          1.41             7.300000 0.70 1.56     750    \n",
       "176 0.43          1.35            10.200000 0.59 1.56     835    \n",
       "177 0.53          1.46             9.300000 0.60 1.62     840    \n",
       "178 0.56          1.35             9.200000 0.61 1.60     560    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nrow(wine)\n",
    "wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vEiem que hi ha molt pocs necessitarem fer algun metode de resampling com boostrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Discriminador lineal de Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Type</th><th scope=col>Alcohol</th><th scope=col>Malic</th><th scope=col>Ash</th><th scope=col>Alcalinity</th><th scope=col>Magnesium</th><th scope=col>Phenols</th><th scope=col>Flavonoids</th><th scope=col>Nonflavonoids</th><th scope=col>Proanthocyanins</th><th scope=col>Color</th><th scope=col>Hue</th><th scope=col>Dilution</th><th scope=col>Proline</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1    </td><td>14.23</td><td>1.71 </td><td>2.43 </td><td>15.6 </td><td>127  </td><td>2.80 </td><td>3.06 </td><td>0.28 </td><td>2.29 </td><td>5.64 </td><td>1.04 </td><td>3.92 </td><td>1065 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.20</td><td>1.78 </td><td>2.14 </td><td>11.2 </td><td>100  </td><td>2.65 </td><td>2.76 </td><td>0.26 </td><td>1.28 </td><td>4.38 </td><td>1.05 </td><td>3.40 </td><td>1050 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.16</td><td>2.36 </td><td>2.67 </td><td>18.6 </td><td>101  </td><td>2.80 </td><td>3.24 </td><td>0.30 </td><td>2.81 </td><td>5.68 </td><td>1.03 </td><td>3.17 </td><td>1185 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.37</td><td>1.95 </td><td>2.50 </td><td>16.8 </td><td>113  </td><td>3.85 </td><td>3.49 </td><td>0.24 </td><td>2.18 </td><td>7.80 </td><td>0.86 </td><td>3.45 </td><td>1480 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.24</td><td>2.59 </td><td>2.87 </td><td>21.0 </td><td>118  </td><td>2.80 </td><td>2.69 </td><td>0.39 </td><td>1.82 </td><td>4.32 </td><td>1.04 </td><td>2.93 </td><td> 735 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.20</td><td>1.76 </td><td>2.45 </td><td>15.2 </td><td>112  </td><td>3.27 </td><td>3.39 </td><td>0.34 </td><td>1.97 </td><td>6.75 </td><td>1.05 </td><td>2.85 </td><td>1450 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.39</td><td>1.87 </td><td>2.45 </td><td>14.6 </td><td> 96  </td><td>2.50 </td><td>2.52 </td><td>0.30 </td><td>1.98 </td><td>5.25 </td><td>1.02 </td><td>3.58 </td><td>1290 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.06</td><td>2.15 </td><td>2.61 </td><td>17.6 </td><td>121  </td><td>2.60 </td><td>2.51 </td><td>0.31 </td><td>1.25 </td><td>5.05 </td><td>1.06 </td><td>3.58 </td><td>1295 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.83</td><td>1.64 </td><td>2.17 </td><td>14.0 </td><td> 97  </td><td>2.80 </td><td>2.98 </td><td>0.29 </td><td>1.98 </td><td>5.20 </td><td>1.08 </td><td>2.85 </td><td>1045 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.86</td><td>1.35 </td><td>2.27 </td><td>16.0 </td><td> 98  </td><td>2.98 </td><td>3.15 </td><td>0.22 </td><td>1.85 </td><td>7.22 </td><td>1.01 </td><td>3.55 </td><td>1045 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.10</td><td>2.16 </td><td>2.30 </td><td>18.0 </td><td>105  </td><td>2.95 </td><td>3.32 </td><td>0.22 </td><td>2.38 </td><td>5.75 </td><td>1.25 </td><td>3.17 </td><td>1510 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.12</td><td>1.48 </td><td>2.32 </td><td>16.8 </td><td> 95  </td><td>2.20 </td><td>2.43 </td><td>0.26 </td><td>1.57 </td><td>5.00 </td><td>1.17 </td><td>2.82 </td><td>1280 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.75</td><td>1.73 </td><td>2.41 </td><td>16.0 </td><td> 89  </td><td>2.60 </td><td>2.76 </td><td>0.29 </td><td>1.81 </td><td>5.60 </td><td>1.15 </td><td>2.90 </td><td>1320 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.75</td><td>1.73 </td><td>2.39 </td><td>11.4 </td><td> 91  </td><td>3.10 </td><td>3.69 </td><td>0.43 </td><td>2.81 </td><td>5.40 </td><td>1.25 </td><td>2.73 </td><td>1150 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.38</td><td>1.87 </td><td>2.38 </td><td>12.0 </td><td>102  </td><td>3.30 </td><td>3.64 </td><td>0.29 </td><td>2.96 </td><td>7.50 </td><td>1.20 </td><td>3.00 </td><td>1547 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.63</td><td>1.81 </td><td>2.70 </td><td>17.2 </td><td>112  </td><td>2.85 </td><td>2.91 </td><td>0.30 </td><td>1.46 </td><td>7.30 </td><td>1.28 </td><td>2.88 </td><td>1310 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.30</td><td>1.92 </td><td>2.72 </td><td>20.0 </td><td>120  </td><td>2.80 </td><td>3.14 </td><td>0.33 </td><td>1.97 </td><td>6.20 </td><td>1.07 </td><td>2.65 </td><td>1280 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.83</td><td>1.57 </td><td>2.62 </td><td>20.0 </td><td>115  </td><td>2.95 </td><td>3.40 </td><td>0.40 </td><td>1.72 </td><td>6.60 </td><td>1.13 </td><td>2.57 </td><td>1130 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.19</td><td>1.59 </td><td>2.48 </td><td>16.5 </td><td>108  </td><td>3.30 </td><td>3.93 </td><td>0.32 </td><td>1.86 </td><td>8.70 </td><td>1.23 </td><td>2.82 </td><td>1680 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.64</td><td>3.10 </td><td>2.56 </td><td>15.2 </td><td>116  </td><td>2.70 </td><td>3.03 </td><td>0.17 </td><td>1.66 </td><td>5.10 </td><td>0.96 </td><td>3.36 </td><td> 845 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.06</td><td>1.63 </td><td>2.28 </td><td>16.0 </td><td>126  </td><td>3.00 </td><td>3.17 </td><td>0.24 </td><td>2.10 </td><td>5.65 </td><td>1.09 </td><td>3.71 </td><td> 780 </td></tr>\n",
       "\t<tr><td>1    </td><td>12.93</td><td>3.80 </td><td>2.65 </td><td>18.6 </td><td>102  </td><td>2.41 </td><td>2.41 </td><td>0.25 </td><td>1.98 </td><td>4.50 </td><td>1.03 </td><td>3.52 </td><td> 770 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.71</td><td>1.86 </td><td>2.36 </td><td>16.6 </td><td>101  </td><td>2.61 </td><td>2.88 </td><td>0.27 </td><td>1.69 </td><td>3.80 </td><td>1.11 </td><td>4.00 </td><td>1035 </td></tr>\n",
       "\t<tr><td>1    </td><td>12.85</td><td>1.60 </td><td>2.52 </td><td>17.8 </td><td> 95  </td><td>2.48 </td><td>2.37 </td><td>0.26 </td><td>1.46 </td><td>3.93 </td><td>1.09 </td><td>3.63 </td><td>1015 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.50</td><td>1.81 </td><td>2.61 </td><td>20.0 </td><td> 96  </td><td>2.53 </td><td>2.61 </td><td>0.28 </td><td>1.66 </td><td>3.52 </td><td>1.12 </td><td>3.82 </td><td> 845 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.05</td><td>2.05 </td><td>3.22 </td><td>25.0 </td><td>124  </td><td>2.63 </td><td>2.68 </td><td>0.47 </td><td>1.92 </td><td>3.58 </td><td>1.13 </td><td>3.20 </td><td> 830 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.39</td><td>1.77 </td><td>2.62 </td><td>16.1 </td><td> 93  </td><td>2.85 </td><td>2.94 </td><td>0.34 </td><td>1.45 </td><td>4.80 </td><td>0.92 </td><td>3.22 </td><td>1195 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.30</td><td>1.72 </td><td>2.14 </td><td>17.0 </td><td> 94  </td><td>2.40 </td><td>2.19 </td><td>0.27 </td><td>1.35 </td><td>3.95 </td><td>1.02 </td><td>2.77 </td><td>1285 </td></tr>\n",
       "\t<tr><td>1    </td><td>13.87</td><td>1.90 </td><td>2.80 </td><td>19.4 </td><td>107  </td><td>2.95 </td><td>2.97 </td><td>0.37 </td><td>1.76 </td><td>4.50 </td><td>1.25 </td><td>3.40 </td><td> 915 </td></tr>\n",
       "\t<tr><td>1    </td><td>14.02</td><td>1.68 </td><td>2.21 </td><td>16.0 </td><td> 96  </td><td>2.65 </td><td>2.33 </td><td>0.26 </td><td>1.98 </td><td>4.70 </td><td>1.04 </td><td>3.59 </td><td>1035 </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>3        </td><td>13.32    </td><td>3.24     </td><td>2.38     </td><td>21.5     </td><td> 92      </td><td>1.93     </td><td>0.76     </td><td>0.45     </td><td>1.25     </td><td> 8.420000</td><td>0.55     </td><td>1.62     </td><td>650      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.08    </td><td>3.90     </td><td>2.36     </td><td>21.5     </td><td>113      </td><td>1.41     </td><td>1.39     </td><td>0.34     </td><td>1.14     </td><td> 9.400000</td><td>0.57     </td><td>1.33     </td><td>550      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.50    </td><td>3.12     </td><td>2.62     </td><td>24.0     </td><td>123      </td><td>1.40     </td><td>1.57     </td><td>0.22     </td><td>1.25     </td><td> 8.600000</td><td>0.59     </td><td>1.30     </td><td>500      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.79    </td><td>2.67     </td><td>2.48     </td><td>22.0     </td><td>112      </td><td>1.48     </td><td>1.36     </td><td>0.24     </td><td>1.26     </td><td>10.800000</td><td>0.48     </td><td>1.47     </td><td>480      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.11    </td><td>1.90     </td><td>2.75     </td><td>25.5     </td><td>116      </td><td>2.20     </td><td>1.28     </td><td>0.26     </td><td>1.56     </td><td> 7.100000</td><td>0.61     </td><td>1.33     </td><td>425      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.23    </td><td>3.30     </td><td>2.28     </td><td>18.5     </td><td> 98      </td><td>1.80     </td><td>0.83     </td><td>0.61     </td><td>1.87     </td><td>10.520000</td><td>0.56     </td><td>1.51     </td><td>675      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.58    </td><td>1.29     </td><td>2.10     </td><td>20.0     </td><td>103      </td><td>1.48     </td><td>0.58     </td><td>0.53     </td><td>1.40     </td><td> 7.600000</td><td>0.58     </td><td>1.55     </td><td>640      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.17    </td><td>5.19     </td><td>2.32     </td><td>22.0     </td><td> 93      </td><td>1.74     </td><td>0.63     </td><td>0.61     </td><td>1.55     </td><td> 7.900000</td><td>0.60     </td><td>1.48     </td><td>725      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.84    </td><td>4.12     </td><td>2.38     </td><td>19.5     </td><td> 89      </td><td>1.80     </td><td>0.83     </td><td>0.48     </td><td>1.56     </td><td> 9.010000</td><td>0.57     </td><td>1.64     </td><td>480      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.45    </td><td>3.03     </td><td>2.64     </td><td>27.0     </td><td> 97      </td><td>1.90     </td><td>0.58     </td><td>0.63     </td><td>1.14     </td><td> 7.500000</td><td>0.67     </td><td>1.73     </td><td>880      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.34    </td><td>1.68     </td><td>2.70     </td><td>25.0     </td><td> 98      </td><td>2.80     </td><td>1.31     </td><td>0.53     </td><td>2.70     </td><td>13.000000</td><td>0.57     </td><td>1.96     </td><td>660      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.48    </td><td>1.67     </td><td>2.64     </td><td>22.5     </td><td> 89      </td><td>2.60     </td><td>1.10     </td><td>0.52     </td><td>2.29     </td><td>11.750000</td><td>0.57     </td><td>1.78     </td><td>620      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.36    </td><td>3.83     </td><td>2.38     </td><td>21.0     </td><td> 88      </td><td>2.30     </td><td>0.92     </td><td>0.50     </td><td>1.04     </td><td> 7.650000</td><td>0.56     </td><td>1.58     </td><td>520      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.69    </td><td>3.26     </td><td>2.54     </td><td>20.0     </td><td>107      </td><td>1.83     </td><td>0.56     </td><td>0.50     </td><td>0.80     </td><td> 5.880000</td><td>0.96     </td><td>1.82     </td><td>680      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.85    </td><td>3.27     </td><td>2.58     </td><td>22.0     </td><td>106      </td><td>1.65     </td><td>0.60     </td><td>0.60     </td><td>0.96     </td><td> 5.580000</td><td>0.87     </td><td>2.11     </td><td>570      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.96    </td><td>3.45     </td><td>2.35     </td><td>18.5     </td><td>106      </td><td>1.39     </td><td>0.70     </td><td>0.40     </td><td>0.94     </td><td> 5.280000</td><td>0.68     </td><td>1.75     </td><td>675      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.78    </td><td>2.76     </td><td>2.30     </td><td>22.0     </td><td> 90      </td><td>1.35     </td><td>0.68     </td><td>0.41     </td><td>1.03     </td><td> 9.580000</td><td>0.70     </td><td>1.68     </td><td>615      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.73    </td><td>4.36     </td><td>2.26     </td><td>22.5     </td><td> 88      </td><td>1.28     </td><td>0.47     </td><td>0.52     </td><td>1.15     </td><td> 6.620000</td><td>0.78     </td><td>1.75     </td><td>520      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.45    </td><td>3.70     </td><td>2.60     </td><td>23.0     </td><td>111      </td><td>1.70     </td><td>0.92     </td><td>0.43     </td><td>1.46     </td><td>10.680000</td><td>0.85     </td><td>1.56     </td><td>695      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.82    </td><td>3.37     </td><td>2.30     </td><td>19.5     </td><td> 88      </td><td>1.48     </td><td>0.66     </td><td>0.40     </td><td>0.97     </td><td>10.260000</td><td>0.72     </td><td>1.75     </td><td>685      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.58    </td><td>2.58     </td><td>2.69     </td><td>24.5     </td><td>105      </td><td>1.55     </td><td>0.84     </td><td>0.39     </td><td>1.54     </td><td> 8.660000</td><td>0.74     </td><td>1.80     </td><td>750      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.40    </td><td>4.60     </td><td>2.86     </td><td>25.0     </td><td>112      </td><td>1.98     </td><td>0.96     </td><td>0.27     </td><td>1.11     </td><td> 8.500000</td><td>0.67     </td><td>1.92     </td><td>630      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.20    </td><td>3.03     </td><td>2.32     </td><td>19.0     </td><td> 96      </td><td>1.25     </td><td>0.49     </td><td>0.40     </td><td>0.73     </td><td> 5.500000</td><td>0.66     </td><td>1.83     </td><td>510      </td></tr>\n",
       "\t<tr><td>3        </td><td>12.77    </td><td>2.39     </td><td>2.28     </td><td>19.5     </td><td> 86      </td><td>1.39     </td><td>0.51     </td><td>0.48     </td><td>0.64     </td><td> 9.899999</td><td>0.57     </td><td>1.63     </td><td>470      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.16    </td><td>2.51     </td><td>2.48     </td><td>20.0     </td><td> 91      </td><td>1.68     </td><td>0.70     </td><td>0.44     </td><td>1.24     </td><td> 9.700000</td><td>0.62     </td><td>1.71     </td><td>660      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.71    </td><td>5.65     </td><td>2.45     </td><td>20.5     </td><td> 95      </td><td>1.68     </td><td>0.61     </td><td>0.52     </td><td>1.06     </td><td> 7.700000</td><td>0.64     </td><td>1.74     </td><td>740      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.40    </td><td>3.91     </td><td>2.48     </td><td>23.0     </td><td>102      </td><td>1.80     </td><td>0.75     </td><td>0.43     </td><td>1.41     </td><td> 7.300000</td><td>0.70     </td><td>1.56     </td><td>750      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.27    </td><td>4.28     </td><td>2.26     </td><td>20.0     </td><td>120      </td><td>1.59     </td><td>0.69     </td><td>0.43     </td><td>1.35     </td><td>10.200000</td><td>0.59     </td><td>1.56     </td><td>835      </td></tr>\n",
       "\t<tr><td>3        </td><td>13.17    </td><td>2.59     </td><td>2.37     </td><td>20.0     </td><td>120      </td><td>1.65     </td><td>0.68     </td><td>0.53     </td><td>1.46     </td><td> 9.300000</td><td>0.60     </td><td>1.62     </td><td>840      </td></tr>\n",
       "\t<tr><td>3        </td><td>14.13    </td><td>4.10     </td><td>2.74     </td><td>24.5     </td><td> 96      </td><td>2.05     </td><td>0.76     </td><td>0.56     </td><td>1.35     </td><td> 9.200000</td><td>0.61     </td><td>1.60     </td><td>560      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllllll}\n",
       " Type & Alcohol & Malic & Ash & Alcalinity & Magnesium & Phenols & Flavonoids & Nonflavonoids & Proanthocyanins & Color & Hue & Dilution & Proline\\\\\n",
       "\\hline\n",
       "\t 1     & 14.23 & 1.71  & 2.43  & 15.6  & 127   & 2.80  & 3.06  & 0.28  & 2.29  & 5.64  & 1.04  & 3.92  & 1065 \\\\\n",
       "\t 1     & 13.20 & 1.78  & 2.14  & 11.2  & 100   & 2.65  & 2.76  & 0.26  & 1.28  & 4.38  & 1.05  & 3.40  & 1050 \\\\\n",
       "\t 1     & 13.16 & 2.36  & 2.67  & 18.6  & 101   & 2.80  & 3.24  & 0.30  & 2.81  & 5.68  & 1.03  & 3.17  & 1185 \\\\\n",
       "\t 1     & 14.37 & 1.95  & 2.50  & 16.8  & 113   & 3.85  & 3.49  & 0.24  & 2.18  & 7.80  & 0.86  & 3.45  & 1480 \\\\\n",
       "\t 1     & 13.24 & 2.59  & 2.87  & 21.0  & 118   & 2.80  & 2.69  & 0.39  & 1.82  & 4.32  & 1.04  & 2.93  &  735 \\\\\n",
       "\t 1     & 14.20 & 1.76  & 2.45  & 15.2  & 112   & 3.27  & 3.39  & 0.34  & 1.97  & 6.75  & 1.05  & 2.85  & 1450 \\\\\n",
       "\t 1     & 14.39 & 1.87  & 2.45  & 14.6  &  96   & 2.50  & 2.52  & 0.30  & 1.98  & 5.25  & 1.02  & 3.58  & 1290 \\\\\n",
       "\t 1     & 14.06 & 2.15  & 2.61  & 17.6  & 121   & 2.60  & 2.51  & 0.31  & 1.25  & 5.05  & 1.06  & 3.58  & 1295 \\\\\n",
       "\t 1     & 14.83 & 1.64  & 2.17  & 14.0  &  97   & 2.80  & 2.98  & 0.29  & 1.98  & 5.20  & 1.08  & 2.85  & 1045 \\\\\n",
       "\t 1     & 13.86 & 1.35  & 2.27  & 16.0  &  98   & 2.98  & 3.15  & 0.22  & 1.85  & 7.22  & 1.01  & 3.55  & 1045 \\\\\n",
       "\t 1     & 14.10 & 2.16  & 2.30  & 18.0  & 105   & 2.95  & 3.32  & 0.22  & 2.38  & 5.75  & 1.25  & 3.17  & 1510 \\\\\n",
       "\t 1     & 14.12 & 1.48  & 2.32  & 16.8  &  95   & 2.20  & 2.43  & 0.26  & 1.57  & 5.00  & 1.17  & 2.82  & 1280 \\\\\n",
       "\t 1     & 13.75 & 1.73  & 2.41  & 16.0  &  89   & 2.60  & 2.76  & 0.29  & 1.81  & 5.60  & 1.15  & 2.90  & 1320 \\\\\n",
       "\t 1     & 14.75 & 1.73  & 2.39  & 11.4  &  91   & 3.10  & 3.69  & 0.43  & 2.81  & 5.40  & 1.25  & 2.73  & 1150 \\\\\n",
       "\t 1     & 14.38 & 1.87  & 2.38  & 12.0  & 102   & 3.30  & 3.64  & 0.29  & 2.96  & 7.50  & 1.20  & 3.00  & 1547 \\\\\n",
       "\t 1     & 13.63 & 1.81  & 2.70  & 17.2  & 112   & 2.85  & 2.91  & 0.30  & 1.46  & 7.30  & 1.28  & 2.88  & 1310 \\\\\n",
       "\t 1     & 14.30 & 1.92  & 2.72  & 20.0  & 120   & 2.80  & 3.14  & 0.33  & 1.97  & 6.20  & 1.07  & 2.65  & 1280 \\\\\n",
       "\t 1     & 13.83 & 1.57  & 2.62  & 20.0  & 115   & 2.95  & 3.40  & 0.40  & 1.72  & 6.60  & 1.13  & 2.57  & 1130 \\\\\n",
       "\t 1     & 14.19 & 1.59  & 2.48  & 16.5  & 108   & 3.30  & 3.93  & 0.32  & 1.86  & 8.70  & 1.23  & 2.82  & 1680 \\\\\n",
       "\t 1     & 13.64 & 3.10  & 2.56  & 15.2  & 116   & 2.70  & 3.03  & 0.17  & 1.66  & 5.10  & 0.96  & 3.36  &  845 \\\\\n",
       "\t 1     & 14.06 & 1.63  & 2.28  & 16.0  & 126   & 3.00  & 3.17  & 0.24  & 2.10  & 5.65  & 1.09  & 3.71  &  780 \\\\\n",
       "\t 1     & 12.93 & 3.80  & 2.65  & 18.6  & 102   & 2.41  & 2.41  & 0.25  & 1.98  & 4.50  & 1.03  & 3.52  &  770 \\\\\n",
       "\t 1     & 13.71 & 1.86  & 2.36  & 16.6  & 101   & 2.61  & 2.88  & 0.27  & 1.69  & 3.80  & 1.11  & 4.00  & 1035 \\\\\n",
       "\t 1     & 12.85 & 1.60  & 2.52  & 17.8  &  95   & 2.48  & 2.37  & 0.26  & 1.46  & 3.93  & 1.09  & 3.63  & 1015 \\\\\n",
       "\t 1     & 13.50 & 1.81  & 2.61  & 20.0  &  96   & 2.53  & 2.61  & 0.28  & 1.66  & 3.52  & 1.12  & 3.82  &  845 \\\\\n",
       "\t 1     & 13.05 & 2.05  & 3.22  & 25.0  & 124   & 2.63  & 2.68  & 0.47  & 1.92  & 3.58  & 1.13  & 3.20  &  830 \\\\\n",
       "\t 1     & 13.39 & 1.77  & 2.62  & 16.1  &  93   & 2.85  & 2.94  & 0.34  & 1.45  & 4.80  & 0.92  & 3.22  & 1195 \\\\\n",
       "\t 1     & 13.30 & 1.72  & 2.14  & 17.0  &  94   & 2.40  & 2.19  & 0.27  & 1.35  & 3.95  & 1.02  & 2.77  & 1285 \\\\\n",
       "\t 1     & 13.87 & 1.90  & 2.80  & 19.4  & 107   & 2.95  & 2.97  & 0.37  & 1.76  & 4.50  & 1.25  & 3.40  &  915 \\\\\n",
       "\t 1     & 14.02 & 1.68  & 2.21  & 16.0  &  96   & 2.65  & 2.33  & 0.26  & 1.98  & 4.70  & 1.04  & 3.59  & 1035 \\\\\n",
       "\t ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ... & ...\\\\\n",
       "\t 3         & 13.32     & 3.24      & 2.38      & 21.5      &  92       & 1.93      & 0.76      & 0.45      & 1.25      &  8.420000 & 0.55      & 1.62      & 650      \\\\\n",
       "\t 3         & 13.08     & 3.90      & 2.36      & 21.5      & 113       & 1.41      & 1.39      & 0.34      & 1.14      &  9.400000 & 0.57      & 1.33      & 550      \\\\\n",
       "\t 3         & 13.50     & 3.12      & 2.62      & 24.0      & 123       & 1.40      & 1.57      & 0.22      & 1.25      &  8.600000 & 0.59      & 1.30      & 500      \\\\\n",
       "\t 3         & 12.79     & 2.67      & 2.48      & 22.0      & 112       & 1.48      & 1.36      & 0.24      & 1.26      & 10.800000 & 0.48      & 1.47      & 480      \\\\\n",
       "\t 3         & 13.11     & 1.90      & 2.75      & 25.5      & 116       & 2.20      & 1.28      & 0.26      & 1.56      &  7.100000 & 0.61      & 1.33      & 425      \\\\\n",
       "\t 3         & 13.23     & 3.30      & 2.28      & 18.5      &  98       & 1.80      & 0.83      & 0.61      & 1.87      & 10.520000 & 0.56      & 1.51      & 675      \\\\\n",
       "\t 3         & 12.58     & 1.29      & 2.10      & 20.0      & 103       & 1.48      & 0.58      & 0.53      & 1.40      &  7.600000 & 0.58      & 1.55      & 640      \\\\\n",
       "\t 3         & 13.17     & 5.19      & 2.32      & 22.0      &  93       & 1.74      & 0.63      & 0.61      & 1.55      &  7.900000 & 0.60      & 1.48      & 725      \\\\\n",
       "\t 3         & 13.84     & 4.12      & 2.38      & 19.5      &  89       & 1.80      & 0.83      & 0.48      & 1.56      &  9.010000 & 0.57      & 1.64      & 480      \\\\\n",
       "\t 3         & 12.45     & 3.03      & 2.64      & 27.0      &  97       & 1.90      & 0.58      & 0.63      & 1.14      &  7.500000 & 0.67      & 1.73      & 880      \\\\\n",
       "\t 3         & 14.34     & 1.68      & 2.70      & 25.0      &  98       & 2.80      & 1.31      & 0.53      & 2.70      & 13.000000 & 0.57      & 1.96      & 660      \\\\\n",
       "\t 3         & 13.48     & 1.67      & 2.64      & 22.5      &  89       & 2.60      & 1.10      & 0.52      & 2.29      & 11.750000 & 0.57      & 1.78      & 620      \\\\\n",
       "\t 3         & 12.36     & 3.83      & 2.38      & 21.0      &  88       & 2.30      & 0.92      & 0.50      & 1.04      &  7.650000 & 0.56      & 1.58      & 520      \\\\\n",
       "\t 3         & 13.69     & 3.26      & 2.54      & 20.0      & 107       & 1.83      & 0.56      & 0.50      & 0.80      &  5.880000 & 0.96      & 1.82      & 680      \\\\\n",
       "\t 3         & 12.85     & 3.27      & 2.58      & 22.0      & 106       & 1.65      & 0.60      & 0.60      & 0.96      &  5.580000 & 0.87      & 2.11      & 570      \\\\\n",
       "\t 3         & 12.96     & 3.45      & 2.35      & 18.5      & 106       & 1.39      & 0.70      & 0.40      & 0.94      &  5.280000 & 0.68      & 1.75      & 675      \\\\\n",
       "\t 3         & 13.78     & 2.76      & 2.30      & 22.0      &  90       & 1.35      & 0.68      & 0.41      & 1.03      &  9.580000 & 0.70      & 1.68      & 615      \\\\\n",
       "\t 3         & 13.73     & 4.36      & 2.26      & 22.5      &  88       & 1.28      & 0.47      & 0.52      & 1.15      &  6.620000 & 0.78      & 1.75      & 520      \\\\\n",
       "\t 3         & 13.45     & 3.70      & 2.60      & 23.0      & 111       & 1.70      & 0.92      & 0.43      & 1.46      & 10.680000 & 0.85      & 1.56      & 695      \\\\\n",
       "\t 3         & 12.82     & 3.37      & 2.30      & 19.5      &  88       & 1.48      & 0.66      & 0.40      & 0.97      & 10.260000 & 0.72      & 1.75      & 685      \\\\\n",
       "\t 3         & 13.58     & 2.58      & 2.69      & 24.5      & 105       & 1.55      & 0.84      & 0.39      & 1.54      &  8.660000 & 0.74      & 1.80      & 750      \\\\\n",
       "\t 3         & 13.40     & 4.60      & 2.86      & 25.0      & 112       & 1.98      & 0.96      & 0.27      & 1.11      &  8.500000 & 0.67      & 1.92      & 630      \\\\\n",
       "\t 3         & 12.20     & 3.03      & 2.32      & 19.0      &  96       & 1.25      & 0.49      & 0.40      & 0.73      &  5.500000 & 0.66      & 1.83      & 510      \\\\\n",
       "\t 3         & 12.77     & 2.39      & 2.28      & 19.5      &  86       & 1.39      & 0.51      & 0.48      & 0.64      &  9.899999 & 0.57      & 1.63      & 470      \\\\\n",
       "\t 3         & 14.16     & 2.51      & 2.48      & 20.0      &  91       & 1.68      & 0.70      & 0.44      & 1.24      &  9.700000 & 0.62      & 1.71      & 660      \\\\\n",
       "\t 3         & 13.71     & 5.65      & 2.45      & 20.5      &  95       & 1.68      & 0.61      & 0.52      & 1.06      &  7.700000 & 0.64      & 1.74      & 740      \\\\\n",
       "\t 3         & 13.40     & 3.91      & 2.48      & 23.0      & 102       & 1.80      & 0.75      & 0.43      & 1.41      &  7.300000 & 0.70      & 1.56      & 750      \\\\\n",
       "\t 3         & 13.27     & 4.28      & 2.26      & 20.0      & 120       & 1.59      & 0.69      & 0.43      & 1.35      & 10.200000 & 0.59      & 1.56      & 835      \\\\\n",
       "\t 3         & 13.17     & 2.59      & 2.37      & 20.0      & 120       & 1.65      & 0.68      & 0.53      & 1.46      &  9.300000 & 0.60      & 1.62      & 840      \\\\\n",
       "\t 3         & 14.13     & 4.10      & 2.74      & 24.5      &  96       & 2.05      & 0.76      & 0.56      & 1.35      &  9.200000 & 0.61      & 1.60      & 560      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Type | Alcohol | Malic | Ash | Alcalinity | Magnesium | Phenols | Flavonoids | Nonflavonoids | Proanthocyanins | Color | Hue | Dilution | Proline | \n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1     | 14.23 | 1.71  | 2.43  | 15.6  | 127   | 2.80  | 3.06  | 0.28  | 2.29  | 5.64  | 1.04  | 3.92  | 1065  | \n",
       "| 1     | 13.20 | 1.78  | 2.14  | 11.2  | 100   | 2.65  | 2.76  | 0.26  | 1.28  | 4.38  | 1.05  | 3.40  | 1050  | \n",
       "| 1     | 13.16 | 2.36  | 2.67  | 18.6  | 101   | 2.80  | 3.24  | 0.30  | 2.81  | 5.68  | 1.03  | 3.17  | 1185  | \n",
       "| 1     | 14.37 | 1.95  | 2.50  | 16.8  | 113   | 3.85  | 3.49  | 0.24  | 2.18  | 7.80  | 0.86  | 3.45  | 1480  | \n",
       "| 1     | 13.24 | 2.59  | 2.87  | 21.0  | 118   | 2.80  | 2.69  | 0.39  | 1.82  | 4.32  | 1.04  | 2.93  |  735  | \n",
       "| 1     | 14.20 | 1.76  | 2.45  | 15.2  | 112   | 3.27  | 3.39  | 0.34  | 1.97  | 6.75  | 1.05  | 2.85  | 1450  | \n",
       "| 1     | 14.39 | 1.87  | 2.45  | 14.6  |  96   | 2.50  | 2.52  | 0.30  | 1.98  | 5.25  | 1.02  | 3.58  | 1290  | \n",
       "| 1     | 14.06 | 2.15  | 2.61  | 17.6  | 121   | 2.60  | 2.51  | 0.31  | 1.25  | 5.05  | 1.06  | 3.58  | 1295  | \n",
       "| 1     | 14.83 | 1.64  | 2.17  | 14.0  |  97   | 2.80  | 2.98  | 0.29  | 1.98  | 5.20  | 1.08  | 2.85  | 1045  | \n",
       "| 1     | 13.86 | 1.35  | 2.27  | 16.0  |  98   | 2.98  | 3.15  | 0.22  | 1.85  | 7.22  | 1.01  | 3.55  | 1045  | \n",
       "| 1     | 14.10 | 2.16  | 2.30  | 18.0  | 105   | 2.95  | 3.32  | 0.22  | 2.38  | 5.75  | 1.25  | 3.17  | 1510  | \n",
       "| 1     | 14.12 | 1.48  | 2.32  | 16.8  |  95   | 2.20  | 2.43  | 0.26  | 1.57  | 5.00  | 1.17  | 2.82  | 1280  | \n",
       "| 1     | 13.75 | 1.73  | 2.41  | 16.0  |  89   | 2.60  | 2.76  | 0.29  | 1.81  | 5.60  | 1.15  | 2.90  | 1320  | \n",
       "| 1     | 14.75 | 1.73  | 2.39  | 11.4  |  91   | 3.10  | 3.69  | 0.43  | 2.81  | 5.40  | 1.25  | 2.73  | 1150  | \n",
       "| 1     | 14.38 | 1.87  | 2.38  | 12.0  | 102   | 3.30  | 3.64  | 0.29  | 2.96  | 7.50  | 1.20  | 3.00  | 1547  | \n",
       "| 1     | 13.63 | 1.81  | 2.70  | 17.2  | 112   | 2.85  | 2.91  | 0.30  | 1.46  | 7.30  | 1.28  | 2.88  | 1310  | \n",
       "| 1     | 14.30 | 1.92  | 2.72  | 20.0  | 120   | 2.80  | 3.14  | 0.33  | 1.97  | 6.20  | 1.07  | 2.65  | 1280  | \n",
       "| 1     | 13.83 | 1.57  | 2.62  | 20.0  | 115   | 2.95  | 3.40  | 0.40  | 1.72  | 6.60  | 1.13  | 2.57  | 1130  | \n",
       "| 1     | 14.19 | 1.59  | 2.48  | 16.5  | 108   | 3.30  | 3.93  | 0.32  | 1.86  | 8.70  | 1.23  | 2.82  | 1680  | \n",
       "| 1     | 13.64 | 3.10  | 2.56  | 15.2  | 116   | 2.70  | 3.03  | 0.17  | 1.66  | 5.10  | 0.96  | 3.36  |  845  | \n",
       "| 1     | 14.06 | 1.63  | 2.28  | 16.0  | 126   | 3.00  | 3.17  | 0.24  | 2.10  | 5.65  | 1.09  | 3.71  |  780  | \n",
       "| 1     | 12.93 | 3.80  | 2.65  | 18.6  | 102   | 2.41  | 2.41  | 0.25  | 1.98  | 4.50  | 1.03  | 3.52  |  770  | \n",
       "| 1     | 13.71 | 1.86  | 2.36  | 16.6  | 101   | 2.61  | 2.88  | 0.27  | 1.69  | 3.80  | 1.11  | 4.00  | 1035  | \n",
       "| 1     | 12.85 | 1.60  | 2.52  | 17.8  |  95   | 2.48  | 2.37  | 0.26  | 1.46  | 3.93  | 1.09  | 3.63  | 1015  | \n",
       "| 1     | 13.50 | 1.81  | 2.61  | 20.0  |  96   | 2.53  | 2.61  | 0.28  | 1.66  | 3.52  | 1.12  | 3.82  |  845  | \n",
       "| 1     | 13.05 | 2.05  | 3.22  | 25.0  | 124   | 2.63  | 2.68  | 0.47  | 1.92  | 3.58  | 1.13  | 3.20  |  830  | \n",
       "| 1     | 13.39 | 1.77  | 2.62  | 16.1  |  93   | 2.85  | 2.94  | 0.34  | 1.45  | 4.80  | 0.92  | 3.22  | 1195  | \n",
       "| 1     | 13.30 | 1.72  | 2.14  | 17.0  |  94   | 2.40  | 2.19  | 0.27  | 1.35  | 3.95  | 1.02  | 2.77  | 1285  | \n",
       "| 1     | 13.87 | 1.90  | 2.80  | 19.4  | 107   | 2.95  | 2.97  | 0.37  | 1.76  | 4.50  | 1.25  | 3.40  |  915  | \n",
       "| 1     | 14.02 | 1.68  | 2.21  | 16.0  |  96   | 2.65  | 2.33  | 0.26  | 1.98  | 4.70  | 1.04  | 3.59  | 1035  | \n",
       "| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | \n",
       "| 3         | 13.32     | 3.24      | 2.38      | 21.5      |  92       | 1.93      | 0.76      | 0.45      | 1.25      |  8.420000 | 0.55      | 1.62      | 650       | \n",
       "| 3         | 13.08     | 3.90      | 2.36      | 21.5      | 113       | 1.41      | 1.39      | 0.34      | 1.14      |  9.400000 | 0.57      | 1.33      | 550       | \n",
       "| 3         | 13.50     | 3.12      | 2.62      | 24.0      | 123       | 1.40      | 1.57      | 0.22      | 1.25      |  8.600000 | 0.59      | 1.30      | 500       | \n",
       "| 3         | 12.79     | 2.67      | 2.48      | 22.0      | 112       | 1.48      | 1.36      | 0.24      | 1.26      | 10.800000 | 0.48      | 1.47      | 480       | \n",
       "| 3         | 13.11     | 1.90      | 2.75      | 25.5      | 116       | 2.20      | 1.28      | 0.26      | 1.56      |  7.100000 | 0.61      | 1.33      | 425       | \n",
       "| 3         | 13.23     | 3.30      | 2.28      | 18.5      |  98       | 1.80      | 0.83      | 0.61      | 1.87      | 10.520000 | 0.56      | 1.51      | 675       | \n",
       "| 3         | 12.58     | 1.29      | 2.10      | 20.0      | 103       | 1.48      | 0.58      | 0.53      | 1.40      |  7.600000 | 0.58      | 1.55      | 640       | \n",
       "| 3         | 13.17     | 5.19      | 2.32      | 22.0      |  93       | 1.74      | 0.63      | 0.61      | 1.55      |  7.900000 | 0.60      | 1.48      | 725       | \n",
       "| 3         | 13.84     | 4.12      | 2.38      | 19.5      |  89       | 1.80      | 0.83      | 0.48      | 1.56      |  9.010000 | 0.57      | 1.64      | 480       | \n",
       "| 3         | 12.45     | 3.03      | 2.64      | 27.0      |  97       | 1.90      | 0.58      | 0.63      | 1.14      |  7.500000 | 0.67      | 1.73      | 880       | \n",
       "| 3         | 14.34     | 1.68      | 2.70      | 25.0      |  98       | 2.80      | 1.31      | 0.53      | 2.70      | 13.000000 | 0.57      | 1.96      | 660       | \n",
       "| 3         | 13.48     | 1.67      | 2.64      | 22.5      |  89       | 2.60      | 1.10      | 0.52      | 2.29      | 11.750000 | 0.57      | 1.78      | 620       | \n",
       "| 3         | 12.36     | 3.83      | 2.38      | 21.0      |  88       | 2.30      | 0.92      | 0.50      | 1.04      |  7.650000 | 0.56      | 1.58      | 520       | \n",
       "| 3         | 13.69     | 3.26      | 2.54      | 20.0      | 107       | 1.83      | 0.56      | 0.50      | 0.80      |  5.880000 | 0.96      | 1.82      | 680       | \n",
       "| 3         | 12.85     | 3.27      | 2.58      | 22.0      | 106       | 1.65      | 0.60      | 0.60      | 0.96      |  5.580000 | 0.87      | 2.11      | 570       | \n",
       "| 3         | 12.96     | 3.45      | 2.35      | 18.5      | 106       | 1.39      | 0.70      | 0.40      | 0.94      |  5.280000 | 0.68      | 1.75      | 675       | \n",
       "| 3         | 13.78     | 2.76      | 2.30      | 22.0      |  90       | 1.35      | 0.68      | 0.41      | 1.03      |  9.580000 | 0.70      | 1.68      | 615       | \n",
       "| 3         | 13.73     | 4.36      | 2.26      | 22.5      |  88       | 1.28      | 0.47      | 0.52      | 1.15      |  6.620000 | 0.78      | 1.75      | 520       | \n",
       "| 3         | 13.45     | 3.70      | 2.60      | 23.0      | 111       | 1.70      | 0.92      | 0.43      | 1.46      | 10.680000 | 0.85      | 1.56      | 695       | \n",
       "| 3         | 12.82     | 3.37      | 2.30      | 19.5      |  88       | 1.48      | 0.66      | 0.40      | 0.97      | 10.260000 | 0.72      | 1.75      | 685       | \n",
       "| 3         | 13.58     | 2.58      | 2.69      | 24.5      | 105       | 1.55      | 0.84      | 0.39      | 1.54      |  8.660000 | 0.74      | 1.80      | 750       | \n",
       "| 3         | 13.40     | 4.60      | 2.86      | 25.0      | 112       | 1.98      | 0.96      | 0.27      | 1.11      |  8.500000 | 0.67      | 1.92      | 630       | \n",
       "| 3         | 12.20     | 3.03      | 2.32      | 19.0      |  96       | 1.25      | 0.49      | 0.40      | 0.73      |  5.500000 | 0.66      | 1.83      | 510       | \n",
       "| 3         | 12.77     | 2.39      | 2.28      | 19.5      |  86       | 1.39      | 0.51      | 0.48      | 0.64      |  9.899999 | 0.57      | 1.63      | 470       | \n",
       "| 3         | 14.16     | 2.51      | 2.48      | 20.0      |  91       | 1.68      | 0.70      | 0.44      | 1.24      |  9.700000 | 0.62      | 1.71      | 660       | \n",
       "| 3         | 13.71     | 5.65      | 2.45      | 20.5      |  95       | 1.68      | 0.61      | 0.52      | 1.06      |  7.700000 | 0.64      | 1.74      | 740       | \n",
       "| 3         | 13.40     | 3.91      | 2.48      | 23.0      | 102       | 1.80      | 0.75      | 0.43      | 1.41      |  7.300000 | 0.70      | 1.56      | 750       | \n",
       "| 3         | 13.27     | 4.28      | 2.26      | 20.0      | 120       | 1.59      | 0.69      | 0.43      | 1.35      | 10.200000 | 0.59      | 1.56      | 835       | \n",
       "| 3         | 13.17     | 2.59      | 2.37      | 20.0      | 120       | 1.65      | 0.68      | 0.53      | 1.46      |  9.300000 | 0.60      | 1.62      | 840       | \n",
       "| 3         | 14.13     | 4.10      | 2.74      | 24.5      |  96       | 2.05      | 0.76      | 0.56      | 1.35      |  9.200000 | 0.61      | 1.60      | 560       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "    Type Alcohol Malic Ash  Alcalinity Magnesium Phenols Flavonoids\n",
       "1   1    14.23   1.71  2.43 15.6       127       2.80    3.06      \n",
       "2   1    13.20   1.78  2.14 11.2       100       2.65    2.76      \n",
       "3   1    13.16   2.36  2.67 18.6       101       2.80    3.24      \n",
       "4   1    14.37   1.95  2.50 16.8       113       3.85    3.49      \n",
       "5   1    13.24   2.59  2.87 21.0       118       2.80    2.69      \n",
       "6   1    14.20   1.76  2.45 15.2       112       3.27    3.39      \n",
       "7   1    14.39   1.87  2.45 14.6        96       2.50    2.52      \n",
       "8   1    14.06   2.15  2.61 17.6       121       2.60    2.51      \n",
       "9   1    14.83   1.64  2.17 14.0        97       2.80    2.98      \n",
       "10  1    13.86   1.35  2.27 16.0        98       2.98    3.15      \n",
       "11  1    14.10   2.16  2.30 18.0       105       2.95    3.32      \n",
       "12  1    14.12   1.48  2.32 16.8        95       2.20    2.43      \n",
       "13  1    13.75   1.73  2.41 16.0        89       2.60    2.76      \n",
       "14  1    14.75   1.73  2.39 11.4        91       3.10    3.69      \n",
       "15  1    14.38   1.87  2.38 12.0       102       3.30    3.64      \n",
       "16  1    13.63   1.81  2.70 17.2       112       2.85    2.91      \n",
       "17  1    14.30   1.92  2.72 20.0       120       2.80    3.14      \n",
       "18  1    13.83   1.57  2.62 20.0       115       2.95    3.40      \n",
       "19  1    14.19   1.59  2.48 16.5       108       3.30    3.93      \n",
       "20  1    13.64   3.10  2.56 15.2       116       2.70    3.03      \n",
       "21  1    14.06   1.63  2.28 16.0       126       3.00    3.17      \n",
       "22  1    12.93   3.80  2.65 18.6       102       2.41    2.41      \n",
       "23  1    13.71   1.86  2.36 16.6       101       2.61    2.88      \n",
       "24  1    12.85   1.60  2.52 17.8        95       2.48    2.37      \n",
       "25  1    13.50   1.81  2.61 20.0        96       2.53    2.61      \n",
       "26  1    13.05   2.05  3.22 25.0       124       2.63    2.68      \n",
       "27  1    13.39   1.77  2.62 16.1        93       2.85    2.94      \n",
       "28  1    13.30   1.72  2.14 17.0        94       2.40    2.19      \n",
       "29  1    13.87   1.90  2.80 19.4       107       2.95    2.97      \n",
       "30  1    14.02   1.68  2.21 16.0        96       2.65    2.33      \n",
       "... ...  ...     ...   ...  ...        ...       ...     ...       \n",
       "149 3    13.32   3.24  2.38 21.5        92       1.93    0.76      \n",
       "150 3    13.08   3.90  2.36 21.5       113       1.41    1.39      \n",
       "151 3    13.50   3.12  2.62 24.0       123       1.40    1.57      \n",
       "152 3    12.79   2.67  2.48 22.0       112       1.48    1.36      \n",
       "153 3    13.11   1.90  2.75 25.5       116       2.20    1.28      \n",
       "154 3    13.23   3.30  2.28 18.5        98       1.80    0.83      \n",
       "155 3    12.58   1.29  2.10 20.0       103       1.48    0.58      \n",
       "156 3    13.17   5.19  2.32 22.0        93       1.74    0.63      \n",
       "157 3    13.84   4.12  2.38 19.5        89       1.80    0.83      \n",
       "158 3    12.45   3.03  2.64 27.0        97       1.90    0.58      \n",
       "159 3    14.34   1.68  2.70 25.0        98       2.80    1.31      \n",
       "160 3    13.48   1.67  2.64 22.5        89       2.60    1.10      \n",
       "161 3    12.36   3.83  2.38 21.0        88       2.30    0.92      \n",
       "162 3    13.69   3.26  2.54 20.0       107       1.83    0.56      \n",
       "163 3    12.85   3.27  2.58 22.0       106       1.65    0.60      \n",
       "164 3    12.96   3.45  2.35 18.5       106       1.39    0.70      \n",
       "165 3    13.78   2.76  2.30 22.0        90       1.35    0.68      \n",
       "166 3    13.73   4.36  2.26 22.5        88       1.28    0.47      \n",
       "167 3    13.45   3.70  2.60 23.0       111       1.70    0.92      \n",
       "168 3    12.82   3.37  2.30 19.5        88       1.48    0.66      \n",
       "169 3    13.58   2.58  2.69 24.5       105       1.55    0.84      \n",
       "170 3    13.40   4.60  2.86 25.0       112       1.98    0.96      \n",
       "171 3    12.20   3.03  2.32 19.0        96       1.25    0.49      \n",
       "172 3    12.77   2.39  2.28 19.5        86       1.39    0.51      \n",
       "173 3    14.16   2.51  2.48 20.0        91       1.68    0.70      \n",
       "174 3    13.71   5.65  2.45 20.5        95       1.68    0.61      \n",
       "175 3    13.40   3.91  2.48 23.0       102       1.80    0.75      \n",
       "176 3    13.27   4.28  2.26 20.0       120       1.59    0.69      \n",
       "177 3    13.17   2.59  2.37 20.0       120       1.65    0.68      \n",
       "178 3    14.13   4.10  2.74 24.5        96       2.05    0.76      \n",
       "    Nonflavonoids Proanthocyanins Color     Hue  Dilution Proline\n",
       "1   0.28          2.29            5.64      1.04 3.92     1065   \n",
       "2   0.26          1.28            4.38      1.05 3.40     1050   \n",
       "3   0.30          2.81            5.68      1.03 3.17     1185   \n",
       "4   0.24          2.18            7.80      0.86 3.45     1480   \n",
       "5   0.39          1.82            4.32      1.04 2.93      735   \n",
       "6   0.34          1.97            6.75      1.05 2.85     1450   \n",
       "7   0.30          1.98            5.25      1.02 3.58     1290   \n",
       "8   0.31          1.25            5.05      1.06 3.58     1295   \n",
       "9   0.29          1.98            5.20      1.08 2.85     1045   \n",
       "10  0.22          1.85            7.22      1.01 3.55     1045   \n",
       "11  0.22          2.38            5.75      1.25 3.17     1510   \n",
       "12  0.26          1.57            5.00      1.17 2.82     1280   \n",
       "13  0.29          1.81            5.60      1.15 2.90     1320   \n",
       "14  0.43          2.81            5.40      1.25 2.73     1150   \n",
       "15  0.29          2.96            7.50      1.20 3.00     1547   \n",
       "16  0.30          1.46            7.30      1.28 2.88     1310   \n",
       "17  0.33          1.97            6.20      1.07 2.65     1280   \n",
       "18  0.40          1.72            6.60      1.13 2.57     1130   \n",
       "19  0.32          1.86            8.70      1.23 2.82     1680   \n",
       "20  0.17          1.66            5.10      0.96 3.36      845   \n",
       "21  0.24          2.10            5.65      1.09 3.71      780   \n",
       "22  0.25          1.98            4.50      1.03 3.52      770   \n",
       "23  0.27          1.69            3.80      1.11 4.00     1035   \n",
       "24  0.26          1.46            3.93      1.09 3.63     1015   \n",
       "25  0.28          1.66            3.52      1.12 3.82      845   \n",
       "26  0.47          1.92            3.58      1.13 3.20      830   \n",
       "27  0.34          1.45            4.80      0.92 3.22     1195   \n",
       "28  0.27          1.35            3.95      1.02 2.77     1285   \n",
       "29  0.37          1.76            4.50      1.25 3.40      915   \n",
       "30  0.26          1.98            4.70      1.04 3.59     1035   \n",
       "... ...           ...             ...       ...  ...      ...    \n",
       "149 0.45          1.25             8.420000 0.55 1.62     650    \n",
       "150 0.34          1.14             9.400000 0.57 1.33     550    \n",
       "151 0.22          1.25             8.600000 0.59 1.30     500    \n",
       "152 0.24          1.26            10.800000 0.48 1.47     480    \n",
       "153 0.26          1.56             7.100000 0.61 1.33     425    \n",
       "154 0.61          1.87            10.520000 0.56 1.51     675    \n",
       "155 0.53          1.40             7.600000 0.58 1.55     640    \n",
       "156 0.61          1.55             7.900000 0.60 1.48     725    \n",
       "157 0.48          1.56             9.010000 0.57 1.64     480    \n",
       "158 0.63          1.14             7.500000 0.67 1.73     880    \n",
       "159 0.53          2.70            13.000000 0.57 1.96     660    \n",
       "160 0.52          2.29            11.750000 0.57 1.78     620    \n",
       "161 0.50          1.04             7.650000 0.56 1.58     520    \n",
       "162 0.50          0.80             5.880000 0.96 1.82     680    \n",
       "163 0.60          0.96             5.580000 0.87 2.11     570    \n",
       "164 0.40          0.94             5.280000 0.68 1.75     675    \n",
       "165 0.41          1.03             9.580000 0.70 1.68     615    \n",
       "166 0.52          1.15             6.620000 0.78 1.75     520    \n",
       "167 0.43          1.46            10.680000 0.85 1.56     695    \n",
       "168 0.40          0.97            10.260000 0.72 1.75     685    \n",
       "169 0.39          1.54             8.660000 0.74 1.80     750    \n",
       "170 0.27          1.11             8.500000 0.67 1.92     630    \n",
       "171 0.40          0.73             5.500000 0.66 1.83     510    \n",
       "172 0.48          0.64             9.899999 0.57 1.63     470    \n",
       "173 0.44          1.24             9.700000 0.62 1.71     660    \n",
       "174 0.52          1.06             7.700000 0.64 1.74     740    \n",
       "175 0.43          1.41             7.300000 0.70 1.56     750    \n",
       "176 0.43          1.35            10.200000 0.59 1.56     835    \n",
       "177 0.53          1.46             9.300000 0.60 1.62     840    \n",
       "178 0.56          1.35             9.200000 0.61 1.60     560    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(MASS)\n",
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in table(True = wine.test$Type, Predicted = wine.pred$class): all arguments must have the same length\n",
     "output_type": "error",
     "traceback": [
      "Error in table(True = wine.test$Type, Predicted = wine.pred$class): all arguments must have the same length\nTraceback:\n",
      "1. table(True = wine.test$Type, Predicted = wine.pred$class)   # at line 7 of file <text>",
      "2. stop(\"all arguments must have the same length\")"
     ]
    }
   ],
   "source": [
    "for (i in 1:10){\n",
    "    n<- nrow(wine)\n",
    "    iTrain<-sample(1:n,n,replace = TRUE)\n",
    "    wine.train<-wine[iTrain,]\n",
    "    wine.test<-wine[-iTrain,]\n",
    "    wine.lda1<-lda(Type~.,data=wine.train)\n",
    "    C<-table(\"True\"=wine.test$Type,\"Predicted\"=wine.pred$class)\n",
    "    cat(\"\\n length iTest \", nrow(wine.test),'\\n')\n",
    "    print(C)\n",
    "    cat(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
